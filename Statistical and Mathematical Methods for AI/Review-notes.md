# Numerical computation and finite numbers

## Solving a problem

Usually, to solve a problem, we can enumerate some simple steps:

- Developing a **mathematical model**;
- Developing algorithms to compute the **numerical solution**;
- **Implement** these algorithms;
- Run the software to **simulate** the physical process numerically;
- **Graphically visualize** the results;
- **Interpret** and **validate** the results.

## Sources of approximation

Generally speaking, 4 types of errors can happen during the previously cited process:

- **Measure errors**: caused by the measure instrument;
- **Arithmetic errors**: the rounding errors linked to **each operation** get propagated during the algorithmic process;
- **Truncation errors**: an infinite procedure gets **truncated** to a finite one, causing loss of informations;
- **Inherent errors**: the finite representation of data might lead to loss of informations.

## Measuring error

We can talk about **absolute error** and **relative error**, the latter being the first one divided by the real value:

- **Absolute error**: <img src="svgs/98d8410703d5a79d12a93e7726ce2b25.svg?invert_in_darkmode" align=middle width=81.209865pt height=22.46574pt/>
- **Relative error**: <img src="svgs/da0fc63cdd1f48dd120e844cf3d73d35.svg?invert_in_darkmode" align=middle width=69.830805pt height=28.424219999999995pt/>, obviously stating that <img src="svgs/c831bfaf6e49a680a4f1250d2dcd6d3f.svg?invert_in_darkmode" align=middle width=39.53185500000001pt height=22.831379999999992pt/>

## Accuracy and precision

**Precision** does not measure error: it simply expresses the number of digits we're using. **Accuracy** does: it is the number of **correct** significant digits. One might ask: **what is a significant digit?** The number <img src="svgs/d0e77e0ae0c927639bbf59b3dd1c524b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=21.95721pt/> is said to approximate <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> to <img src="svgs/2103f85b8b1477f430fc407cad462224.svg?invert_in_darkmode" align=middle width=8.556075000000003pt height=22.831379999999992pt/> significant digits if <img src="svgs/2103f85b8b1477f430fc407cad462224.svg?invert_in_darkmode" align=middle width=8.556075000000003pt height=22.831379999999992pt/> is the **largest non-negative** integer for which <img src="svgs/a8597e9b7f58c1a38e482d33571b2098.svg?invert_in_darkmode" align=middle width=98.48173499999999pt height=34.281719999999986pt/>.

## Data error and computational error

In a computational problem, we can define the **total error** as <img src="svgs/bf78575119f4c27fd0de3d6ddc142359.svg?invert_in_darkmode" align=middle width=84.08697000000001pt height=31.50708000000001pt/>, where <img src="svgs/c41f490710f05d3d7b527a61b284ef00.svg?invert_in_darkmode" align=middle width=11.758230000000003pt height=31.50708000000001pt/> is the approximated function, while <img src="svgs/f84e86b97e20e45cc17d297dc794b3e8.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=22.831379999999992pt/> is the approximated input: it is pretty obvious that the error depends on both the function approximation and the data one. The error inherent to the function one is called **computational error** <img src="svgs/efa4295436c149c1dbde725d48655605.svg?invert_in_darkmode" align=middle width=84.086805pt height=31.50708000000001pt/>, the one inherent to the data is called **propagated data error** <img src="svgs/4b2045f21d7317e34c0a4e46dea9b60a.svg?invert_in_darkmode" align=middle width=84.08697000000001pt height=24.65759999999998pt/>.

We can further divide the computational error into the **truncation error** and the **rounding error**. The first one states the difference between the true result and the one produced by given algorithm using **exact arithmetic** (due to approximations such as truncating infinite series), the latter states the difference between the result produced by given algorithm using exact arithmetic and result produced by the same algorithm using **limited precision** arithmetic (due to inexact representation of real numbers). 

## Sensivity and conditioning

**Sensitivity** and **conditioning** are concerned with propagated data error. **Conditioning** is just a way to quantitatively measure sensitivity. A problem is **sensitive** (or **ill-conditioned**) if the relative change in the solution can be much larger than the one in the input data. In other words, a problem is sensitive when, if we change the input by a small quantity, the result changes a lot.

The condition number is expressed as <img src="svgs/56aceb1fc25eaae3d05d05250ab13a7a.svg?invert_in_darkmode" align=middle width=213.149805pt height=33.20559pt/>.

## Representation of a real number in basis <img src="svgs/8217ed3c32a785f0b5aad4055f432ad8.svg?invert_in_darkmode" align=middle width=10.165650000000005pt height=22.831379999999992pt/>

Given an integer <img src="svgs/2e15dd312a6a8979845225e707722553.svg?invert_in_darkmode" align=middle width=40.302405pt height=22.831379999999992pt/> a real number <img src="svgs/c831bfaf6e49a680a4f1250d2dcd6d3f.svg?invert_in_darkmode" align=middle width=39.53185500000001pt height=22.831379999999992pt/> can be expressed in a unique way as <img src="svgs/38930313ca396f93676c514673537933.svg?invert_in_darkmode" align=middle width=252.99235499999998pt height=26.76201000000001pt/>, where <img src="svgs/9da246aababfb069d71ed489a6fb485e.svg?invert_in_darkmode" align=middle width=154.15537500000002pt height=26.76201000000001pt/> is called **mantissa** <img src="svgs/c1af73a7f18e65d681c0f149e3bbb8ce.svg?invert_in_darkmode" align=middle width=91.26348pt height=27.775769999999994pt/>, and <img src="svgs/afe6fc8ae2245c9133fda046afda6180.svg?invert_in_darkmode" align=middle width=16.942035000000004pt height=22.831379999999992pt/> is the **exponential part**. The **normalized scientific representation** is the one starting with 0: <img src="svgs/376c8f263bd0a1cfb16899629845e0f7.svg?invert_in_darkmode" align=middle width=148.10086500000003pt height=24.65759999999998pt/>, the **mixed representation** has leading zeros.

## Floating point systems

We can therefore define a system of floating point numbers <img src="svgs/4eea8cf647d3dd27deea645edcad2081.svg?invert_in_darkmode" align=middle width=88.45534500000001pt height=24.65759999999998pt/>, where the parameters define the **base** <img src="svgs/8217ed3c32a785f0b5aad4055f432ad8.svg?invert_in_darkmode" align=middle width=10.165650000000005pt height=22.831379999999992pt/> , the **precision** <img src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg?invert_in_darkmode" align=middle width=5.936155500000004pt height=20.222069999999988pt/>, and the exponent range <img src="svgs/d71369fd4630ca9ce95b2c4db8960a33.svg?invert_in_darkmode" align=middle width=40.641645000000004pt height=24.65759999999998pt/>. The system is **normalized** when it has no leading zeros: <img src="svgs/e0fec874e861bbf6bd95ae5bdd8df9e3.svg?invert_in_darkmode" align=middle width=46.06734pt height=22.831379999999992pt/>. Why bother to do so? The representation of each number is **unique**, we don't waste digits and the leading bit doesn't have to be stored (binary-lly speaking).

### Properties of floating point systems

A floating point number system is **finite** and **discrete**. The total number of normalized floating point numbers is given by <img src="svgs/27c7240a60b85d2886098bd47f317805.svg?invert_in_darkmode" align=middle width=205.960755pt height=26.76201000000001pt/>. The **smallest positive normalized** number, aka the **UnderFlow Level** is <img src="svgs/36fda4016eff3e1731562da57620d6b3.svg?invert_in_darkmode" align=middle width=78.158685pt height=27.656969999999987pt/>, while the largest number, aka the **OverFlow Level** is <img src="svgs/622a8011486e58eab93e4d00a5e8cf96.svg?invert_in_darkmode" align=middle width=164.09695499999998pt height=27.656969999999987pt/>. Floating point numbers are equally spaced only between successive powers of <img src="svgs/8217ed3c32a785f0b5aad4055f432ad8.svg?invert_in_darkmode" align=middle width=10.165650000000005pt height=22.831379999999992pt/>: **not all real numbers are representable**, just the **machine numbers**, i.e. elements of <img src="svgs/4eea8cf647d3dd27deea645edcad2081.svg?invert_in_darkmode" align=middle width=88.45534500000001pt height=24.65759999999998pt/>.

## Rounding rules

Since not all real numbers are representable, we have to find a way of approximating them to a floating point number <img src="svgs/e6d0263a97a594d025aae07a2798c208.svg?invert_in_darkmode" align=middle width=70.76487pt height=24.65759999999998pt/>. Usually, two strategies are used: **chop**, which truncates the number after the <img src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg?invert_in_darkmode" align=middle width=5.936155500000004pt height=20.222069999999988pt/>-st digit, and **round to nearest** (aka *round to even*), using the floating point number whose last stored digit is even in case of a tie. **Round to nearest is the most accurate**: to state so, we define the accuracy characterized by *unit roundoff*, denoted by <img src="svgs/f03b6d41ffe5cc356176afeab3f2680e.svg?invert_in_darkmode" align=middle width=39.03834pt height=14.155350000000013pt/>. The first rounding rule has a <img src="svgs/1703c0b1f11080da989fad73a383de3c.svg?invert_in_darkmode" align=middle width=93.73584pt height=26.76201000000001pt/>, while the second one <img src="svgs/e14b1820bf9d06df18899511038029e2.svg?invert_in_darkmode" align=middle width=104.23347pt height=27.775769999999994pt/>. The difference is pretty easily spottable: <img src="svgs/47d54de4e337a06266c0e1d22c9b417b.svg?invert_in_darkmode" align=middle width=6.552644999999998pt height=27.775769999999994pt/>. We even have an alternative definition for <img src="svgs/f03b6d41ffe5cc356176afeab3f2680e.svg?invert_in_darkmode" align=middle width=39.03834pt height=14.155350000000013pt/>, which is the smallest <img src="svgs/7ccca27b5ccc533a2dd72dc6fa28ed84.svg?invert_in_darkmode" align=middle width=6.672451500000003pt height=14.155350000000013pt/> such that <img src="svgs/9ec029ef57e67a76c54a1d3d15c97e0d.svg?invert_in_darkmode" align=middle width=92.950935pt height=24.65759999999998pt/>.

We know that the maximum relative error in representing a real number x is always lesser or equal to <img src="svgs/aaa62263c41d42f00da2eab65d60334c.svg?invert_in_darkmode" align=middle width=40.02009pt height=14.155350000000013pt/>: <img src="svgs/d6a0265a2d3c5ddf8919996eb5a74c72.svg?invert_in_darkmode" align=middle width=123.24048pt height=37.80842999999999pt/>.

Remember: **do not confuse <img src="svgs/f03b6d41ffe5cc356176afeab3f2680e.svg?invert_in_darkmode" align=middle width=39.03834pt height=14.155350000000013pt/> with UFL**, the first one is decided by the precision <img src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg?invert_in_darkmode" align=middle width=5.936155500000004pt height=20.222069999999988pt/> of the mantissa, the second one by the minimum exponent <img src="svgs/ddcb483302ed36a59286424aa5e0be17.svg?invert_in_darkmode" align=middle width=11.187330000000003pt height=22.46574pt/>. Therefore, <img src="svgs/3954f67dfa4b417326be68386c393c29.svg?invert_in_darkmode" align=middle width=187.926255pt height=22.46574pt/>.

### Subnormals and gradual underflow

You can easily imagine how normalization causes a gap around zero: to solve this, we can allow leading zeros just when the exponent is at its minimum value. We have filled in the gap with **additional subnormal/denormalized floating point numbers**. These extend the range of representable magnitudes, without increasing the precision. Augmented systems, therefore, exhibit **gradual underflow**.

IEEE has introduced some exceptional values, like <img src="svgs/d8fbc05883bc9ebbcf639b4cdf6bb85f.svg?invert_in_darkmode" align=middle width=64.24621499999999pt height=20.09139000000001pt/> and <img src="svgs/1f31616e14bdba2ef27b7f858f60401f.svg?invert_in_darkmode" align=middle width=25.890315pt height=20.09139000000001pt/>, which stands for *Not a Number*.

## Floating point arithmetic

Results of floating point arithmetic operations might differ from the real ones: in **addition and subtraction**, the shifting of mantissa to match the exponents might cause loss of digits, in **multiplication** the product might contain up to <img src="svgs/b03d2c90e8d8e2b659ff5a34285a73c2.svg?invert_in_darkmode" align=middle width=14.155350000000004pt height=21.18732pt/> digits, in **division** the quotient might contain more than <img src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg?invert_in_darkmode" align=middle width=5.936155500000004pt height=20.222069999999988pt/> digits. Real results may also fail to be representable because the exponent <img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/> is beyond available range. Keep in mind that **overflow is worse than underflow**: ultra small numbers can be rounded to 0, ultra big ones are not <img src="svgs/f7a0f24dc1f54ce82fecccbbf48fca93.svg?invert_in_darkmode" align=middle width=16.438455000000005pt height=14.155350000000013pt/>.

Ideally, we want floating point operations to produce correctly rounded results: <img src="svgs/7725b68af0182eba6d3d16999de98c64.svg?invert_in_darkmode" align=middle width=168.06355499999998pt height=24.65759999999998pt/>. Computers satisfying IEEE standards achieve this ideal, as long as <img src="svgs/ec42c40fec0c601f7e47e78a2589360c.svg?invert_in_darkmode" align=middle width=45.241845pt height=14.155350000000013pt/> is within the range of the floating point system.

### Cancellation

**Cancellation** happens when we subtract two <img src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg?invert_in_darkmode" align=middle width=5.936155500000004pt height=20.222069999999988pt/>-digit numbers having same sign and similar magnitudes, yielding results with fewer than <img src="svgs/4f4f4e395762a3af4575de74c019ebb5.svg?invert_in_darkmode" align=middle width=5.936155500000004pt height=20.222069999999988pt/> digits: leading digits of the two numbers cancel (their difference is <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>).

# Linear algebra basics for AI

## Vector spaces

A **vector space** over a field <img src="svgs/b8bc815b5e9d5177af01fd4d3d3c2f10.svg?invert_in_darkmode" align=middle width=12.853995000000003pt height=22.46574pt/> (which might be, for example, <img src="svgs/f3e711926cecfed3003f9ae341f3d92b.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.64855999999997pt/> or <img src="svgs/81324f07e9ffb7920321df72cc0bee1b.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.64855999999997pt/>) is a set closed under **vector addition** and **scalar multiplication**. The elements of <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> are called **vectors** while the elements of <img src="svgs/b8bc815b5e9d5177af01fd4d3d3c2f10.svg?invert_in_darkmode" align=middle width=12.853995000000003pt height=22.46574pt/> are called **scalars**. The two operations must satisfy the **commutativity and associativity** of addition, the **existence of the identity element** for addition, the **existence of the additive inverse** (<img src="svgs/f6f6858821f6de3e0e0582b3ddb1537e.svg?invert_in_darkmode" align=middle width=46.575374999999994pt height=22.46574pt/>), the **existence of the identity element** for scalar multiplication (<img src="svgs/546cb86004fe0f4c80868528f2a05eb4.svg?invert_in_darkmode" align=middle width=41.164365000000004pt height=22.46574pt/>), the **compatibility of scalar multiplication with field multiplication**, the **distributivity of scalar multiplication** with respect to field addition, the **distributivity of scalar multiplication** with respect to vector addition.

For example some vector spaces might be <img src="svgs/8a86f4a11e2fbfc03de61d587ba826de.svg?invert_in_darkmode" align=middle width=19.998330000000006pt height=22.64855999999997pt/> , <img src="svgs/4397b563e5324f31e1f31a0dcb1c00dd.svg?invert_in_darkmode" align=middle width=19.998330000000006pt height=22.64855999999997pt/>, <img src="svgs/6ee7f3ea71fe37cfe8a9d444203cb1a7.svg?invert_in_darkmode" align=middle width=18.171780000000005pt height=22.64855999999997pt/> (polynomials with degree <img src="svgs/4f733d7c20d8dfac63d05769f79e5ebd.svg?invert_in_darkmode" align=middle width=27.218400000000003pt height=20.908799999999992pt/>).

Given a vector space <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> over the field <img src="svgs/b8bc815b5e9d5177af01fd4d3d3c2f10.svg?invert_in_darkmode" align=middle width=12.853995000000003pt height=22.46574pt/>, the set <img src="svgs/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode" align=middle width=17.808285000000005pt height=22.46574pt/> is a **subspace** of <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> if and only if <img src="svgs/42615d7f0ce1a1b1449cdeef5973ef8b.svg?invert_in_darkmode" align=middle width=52.96797pt height=22.46574pt/> (**subset**) and <img src="svgs/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode" align=middle width=17.808285000000005pt height=22.46574pt/> is a **vector space** over <img src="svgs/b8bc815b5e9d5177af01fd4d3d3c2f10.svg?invert_in_darkmode" align=middle width=12.853995000000003pt height=22.46574pt/>.

## Linear independence

Given a vector space <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> over <img src="svgs/b8bc815b5e9d5177af01fd4d3d3c2f10.svg?invert_in_darkmode" align=middle width=12.853995000000003pt height=22.46574pt/>, the set <img src="svgs/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode" align=middle width=17.808285000000005pt height=22.46574pt/> of all finite linear combinations of vectors <img src="svgs/ac00f6cfcf3e00b4d18b2815a9a8fbac.svg?invert_in_darkmode" align=middle width=80.546235pt height=24.65759999999998pt/>, with <img src="svgs/1c5d5e44e91b0e80d8cd0db25441b56f.svg?invert_in_darkmode" align=middle width=46.774035000000005pt height=22.46574pt/> is called the **subspace spanned by** <img src="svgs/e283713453ff07ba6652a2f92f7a1e8b.svg?invert_in_darkmode" align=middle width=100.47114pt height=24.65759999999998pt/> and it is written as <img src="svgs/f0c43409f758db3530677aeeea3c7513.svg?invert_in_darkmode" align=middle width=368.073255pt height=26.438939999999977pt/>. The system <img src="svgs/ac00f6cfcf3e00b4d18b2815a9a8fbac.svg?invert_in_darkmode" align=middle width=80.546235pt height=24.65759999999998pt/> is called a **system of generators** for <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/>. 

Now, given a vector space <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> over <img src="svgs/b8bc815b5e9d5177af01fd4d3d3c2f10.svg?invert_in_darkmode" align=middle width=12.853995000000003pt height=22.46574pt/>, a system of vectors <img src="svgs/ac00f6cfcf3e00b4d18b2815a9a8fbac.svg?invert_in_darkmode" align=middle width=80.546235pt height=24.65759999999998pt/> is said **linearly independent** if <img src="svgs/831f80c2ceefed9586702f135745f431.svg?invert_in_darkmode" align=middle width=380.90530499999994pt height=21.18732pt/> with <img src="svgs/5a379de6b56db6751e2b9716f4178d51.svg?invert_in_darkmode" align=middle width=102.14803500000001pt height=22.46574pt/>. Otherwise, the system is called **linearly dependent**. From a geometrical point of view, vectors are linearly dependent if they lie on the same hyperplane.

We call a **basis** for a vector space <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> any system of linearly independent generators of <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/>.

If a vector space <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> admits a basis of <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> vectors, any other basis of <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> will have exactly <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> elements. <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> is the **dimension** of <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/>, noted by <img src="svgs/843ab97eb2030becd5c05de5f2440a1f.svg?invert_in_darkmode" align=middle width=86.46428999999999pt height=24.65759999999998pt/>.

## Matrices

Let <img src="svgs/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode" align=middle width=14.433210000000003pt height=14.155350000000013pt/> and <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> be two positive integers. We call **matrix** the rectangular array having <img src="svgs/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode" align=middle width=14.433210000000003pt height=14.155350000000013pt/> rows and <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> columns of elements in a field F:

<img src="svgs/764439eb8f2ac8d931ef46049227dd9d.svg?invert_in_darkmode" align=middle width=228.59380499999997pt height=96.98732999999999pt/>

If <img src="svgs/103a36ecf09d49e563cb987384457874.svg?invert_in_darkmode" align=middle width=46.64385000000001pt height=22.64855999999997pt/> we write, for example, <img src="svgs/ad73601e39ef9a2d0bc591935b918895.svg?invert_in_darkmode" align=middle width=74.357085pt height=26.177579999999978pt/>. If <img src="svgs/fa0a66891711f605a7176c8234000201.svg?invert_in_darkmode" align=middle width=46.217655pt height=14.155350000000013pt/> we can say the matrix is **square**. The set of entries where <img src="svgs/cd479494c39d95dc8012b12ea2a67946.svg?invert_in_darkmode" align=middle width=35.291355pt height=21.683310000000006pt/> is the **main diagonal**. The maximum number of linearly independent columns (or rows!) is called **rank**. <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is said to be **complete** or **full rank** if <img src="svgs/368f9b2b5d12434a7a8dedee0f8bb6bf.svg?invert_in_darkmode" align=middle width=156.891405pt height=24.65759999999998pt/>.

A **lower triangular** matrix is a matrix that has elements in the diagonal and under it. An upper triangular matrix has elements in the diagonal and over it.

We can do operations with matrices too! These are the most used: *matrix addition*, *matrix multiplication by a scalar*, *matrix multiplication* (notice that it is defined only when <img src="svgs/0efdbed97466f783a3d89f8b00dd976d.svg?invert_in_darkmode" align=middle width=198.154605pt height=26.177579999999978pt/>), *transposition*. 

A **diagonal matrix** is a matrix having elements on the diagonal only, and 0 elsewhere.

The **identity matrix** is a diagonal matrix having 1s on the diagonal. Note that this is the identity element for the multiplication. 

A matrix <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is called **invertible** (or **nonsingular**) if there exists a matrix <img src="svgs/61e84f854bc6258d4108d08d4c4a0852.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=22.46574pt/> such that <img src="svgs/5e12777a129307ac636e6e8c28c0d5bc.svg?invert_in_darkmode" align=middle width=103.595745pt height=22.46574pt/>. <img src="svgs/61e84f854bc6258d4108d08d4c4a0852.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=22.46574pt/> is called the **inverse** of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> and it is denoted by <img src="svgs/471d65ea6d03a4f1ea1dd8be931d26c9.svg?invert_in_darkmode" align=middle width=29.1555pt height=26.76201000000001pt/>. A non-invertible matrix is said **singular**. The inverse of a matrix is also invertible, and it results in the original matrix. The inverse of a product is the product of the inverses: <img src="svgs/fa93cf6a0623f7e915926d87a603d380.svg?invert_in_darkmode" align=middle width=138.07101000000003pt height=26.76201000000001pt/>. If a square matrix is invertible, then <img src="svgs/6da422975b734006d19c0be96c05afaf.svg?invert_in_darkmode" align=middle width=182.209005pt height=27.656969999999987pt/> .

A **square** matrix is invertible iff its column vectors are linearly independent.

A square matrix is called **symmetric** if the transpose is equal to the original matrix, i.e. <img src="svgs/e2704e851675be21b9ae796d884727c3.svg?invert_in_darkmode" align=middle width=56.93094000000001pt height=27.656969999999987pt/>, **antisymmetric** if <img src="svgs/2ac5ae7e81d9ce5e3403aca69be55085.svg?invert_in_darkmode" align=middle width=68.89443pt height=27.656969999999987pt/>, **orthogonal** if <img src="svgs/0e73d7bb58eaa0b08bb72c5148b796ab.svg?invert_in_darkmode" align=middle width=73.757475pt height=27.656969999999987pt/>. Note that if it is orthogonal, we have that <img src="svgs/aa01fe248afbb34d913e6a93fbea4b22.svg?invert_in_darkmode" align=middle width=122.37769500000002pt height=27.656969999999987pt/>.

### Determinant of a matrix

Let <img src="svgs/4a750ea101ce75a11b3982f2cc3eb8c6.svg?invert_in_darkmode" align=middle width=70.81816500000001pt height=26.177579999999978pt/> be a **square matrix**. We call the **determinant** of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> the scalar defined by 

<img src="svgs/6a79bde4c965e32f0bc5741fe5a0c3e5.svg?invert_in_darkmode" align=middle width=556.641855pt height=47.67179999999999pt/>

which is a fancy way to express that we have to:

- Fix one of the two indices;
- Put a negative sign before the quantities that belong to *indices summing to an odd number*;
- Calculate the determinant of the submatrix obtained by removal of the column and row we are analyzing;
- Multiply this quantity for the actual number at the index;
- Sum all of these.

This procedure is known as **Laplace rule**.

If <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is diagonal or triangular, we just have to **multiply the elements on the diagonal**.

The determinant has some interesting properties:

- **Transposing** the matrix **doesn't change** the determinant: <img src="svgs/2dd9da64ff04d2dae6e5b1bec3b3756b.svg?invert_in_darkmode" align=middle width=126.79408499999998pt height=27.656969999999987pt/> 
- The **multiplication of determinants** is the **determinant of multiplications** in square matrices having the same size: <img src="svgs/f4bc56b311ac4df76e7db7f9103879a8.svg?invert_in_darkmode" align=middle width=177.956955pt height=24.65759999999998pt/>
- The determinant of the **inverse** is <img src="svgs/e5355330b158ce6cf27892bfe544c88f.svg?invert_in_darkmode" align=middle width=38.20575pt height=27.775769999999994pt/> : <img src="svgs/345eb82ef953152406319d0dedc27ebf.svg?invert_in_darkmode" align=middle width=150.91362pt height=26.76201000000001pt/>
- The determinant of a scalar multiplication is the scalar multiplication to the power <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/>, with <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> being the matrix size: <img src="svgs/7360d85f1d4c5197e1278b0f782814c1.svg?invert_in_darkmode" align=middle width=206.49865499999999pt height=24.65759999999998pt/>
- Every **orthogonal matrix** is invertible and its determinant is <img src="svgs/9ae0fd34c5ab292d1ccce6a94d7188d5.svg?invert_in_darkmode" align=middle width=21.004665000000006pt height=21.18732pt/>
- The determinant of a <img src="svgs/5642f62a9faca8d26da9e24171f49747.svg?invert_in_darkmode" align=middle width=36.52968pt height=21.18732pt/> square matrix is computable by subtracting the multiplication of the main diagonal elements to the one of the inverse diagonal: <img src="svgs/87dc94194a164c500945c45d53039428.svg?invert_in_darkmode" align=middle width=179.59705499999998pt height=24.65759999999998pt/>
- The determinant of a <img src="svgs/53e14fe4f3521c64c328f4a15bffeef3.svg?invert_in_darkmode" align=middle width=36.52968pt height=21.18732pt/> matrix is computable with the **Sarrus rule**, which basically is the same principle.

### Calculating the inverse using the determinant

If <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is invertible, then its inverse is computable by calculating the **cofactor matrix**, which contains elements defined by <img src="svgs/4d33f6f9f26d1a1d15c26525e7e48707.svg?invert_in_darkmode" align=middle width=161.069205pt height=27.159000000000013pt/>, called **cofactors**.

<img src="svgs/44114dd91883e2ca53d5fa30a31b9e52.svg?invert_in_darkmode" align=middle width=92.07329999999999pt height=34.09889999999999pt/>

### Eigenvalues and eigenvectors

Let's consider a **square** matrix <img src="svgs/4058b231996dd16d611ab57a6ef400dd.svg?invert_in_darkmode" align=middle width=70.81816500000001pt height=26.177579999999978pt/>. The number <img src="svgs/fd8be73b54f5436a5cd2e73ba9b6bfa9.svg?invert_in_darkmode" align=middle width=9.589140000000002pt height=22.831379999999992pt/> is called an **eigenvalue** of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> if we have a vector <img src="svgs/c1eb784d6e6e1cadb2add784cd9a22a8.svg?invert_in_darkmode" align=middle width=39.53185500000001pt height=22.831379999999992pt/>, named **eigenvector**, such that <img src="svgs/de134c5fee8195ddcb5802524b4cab4e.svg?invert_in_darkmode" align=middle width=62.625585pt height=22.831379999999992pt/>. The set of eigenvalues is called **spectrum** of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> and it is denoted by <img src="svgs/b13aeffb44573c812f4907226de5112f.svg?invert_in_darkmode" align=middle width=35.097150000000006pt height=24.65759999999998pt/>. 

**How can we find these eigenvalues?** They are the solutions of the characteristic equation: <img src="svgs/77216447609e8196e4dbeb8e344495f5.svg?invert_in_darkmode" align=middle width=178.86445500000002pt height=24.65759999999998pt/>, where <img src="svgs/925931f2542689e0259081e9ac0d3ec7.svg?invert_in_darkmode" align=middle width=41.35296pt height=24.65759999999998pt/> is called the **characteristic polynomial**. Thanks to the fundamental theorem of algebra we can say that a matrix with **real or complex** entries has <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> eigenvalues, counted with their multiplicity. The algebraic multiplicity of an eigenvalue <img src="svgs/10b25a8965607b9859b33bd6a26ec73b.svg?invert_in_darkmode" align=middle width=14.239995000000002pt height=22.831379999999992pt/> is the number of times the root appears in the characteristic polynomial. Let's make an example: we have a matrix <img src="svgs/76a1cc4c89adb5db61e2c4a52a2f7eba.svg?invert_in_darkmode" align=middle width=136.98629999999997pt height=67.39788pt/>, and we'll now calculate <img src="svgs/51b37e810ccb12623379c4a0b03b971f.svg?invert_in_darkmode" align=middle width=126.35271000000002pt height=24.65759999999998pt/>:

<img src="svgs/f71b59a9b1e721e8658f3eba2e469167.svg?invert_in_darkmode" align=middle width=320.09075999999993pt height=222.41293799999997pt/>

As you can see, we have two roots <img src="svgs/a727f6d657c0b5c8830432b6130080c7.svg?invert_in_darkmode" align=middle width=36.529845pt height=21.18732pt/>, but <img src="svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/> cancels <img src="svgs/253c305350b3c5dbbd6a3842b0efd203.svg?invert_in_darkmode" align=middle width=18.156600000000005pt height=14.155350000000013pt/> two times, so it will have multiplicity <img src="svgs/76c5792347bb90ef71cfbace628572cf.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>, while <img src="svgs/e11a8cfcf953c683196d7a48677b2277.svg?invert_in_darkmode" align=middle width=21.004665000000006pt height=21.18732pt/> will have multiplicity <img src="svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>.

Note that the sum of the multiplicities **can never** be higher than the matrix dimension.

The maximum eigenvalue **in module** of a matrix <img src="svgs/4058b231996dd16d611ab57a6ef400dd.svg?invert_in_darkmode" align=middle width=70.81816500000001pt height=26.177579999999978pt/> is called the **spectral radius** of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> and is denoted by <img src="svgs/5051e103190d79cfd732c60b72edf4d2.svg?invert_in_darkmode" align=middle width=33.61314000000001pt height=24.65759999999998pt/>: <img src="svgs/c8d04dd1ee846ce5c459ad8c735e34a4.svg?invert_in_darkmode" align=middle width=153.0705pt height=24.65759999999998pt/>. The set of all the eigenvalues is called the **spectrum** of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>.

Please note that the **eigenvectors are not unique**: for example, multiplying all of the eigenvectors by a constant <img src="svgs/3e18a4a28fdee1744e5e3f79d13b9ff6.svg?invert_in_darkmode" align=middle width=7.113876000000004pt height=14.155350000000013pt/> would still make them valid eigenvectors for the same values.

We can **link the eigenvalues to the determinant**! In fact, the determinant of a square matrix is the product of all its eigenvalues: <img src="svgs/29107fc410dccaaf27befb7835e035a9.svg?invert_in_darkmode" align=middle width=124.48458pt height=26.438939999999977pt/>.

A matrix is **singular** iff it has at least **one null eigenvalue**.

**Eigenvalues in triangular and diagonal matrices are ultra easy!** In fact, they are the elements of the diagonal. 

A symmetric (<img src="svgs/e2704e851675be21b9ae796d884727c3.svg?invert_in_darkmode" align=middle width=56.93094000000001pt height=27.656969999999987pt/>) positive (semi)definite matrix has eigenvalues greater(equal) than(to) zero.

Two matrices **with the same size** are said **similar** if the nonsingular matrix <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.836835000000004pt height=22.46574pt/> exists: <img src="svgs/b39d85f7b525362fb5d242227352a03d.svg?invert_in_darkmode" align=middle width=90.04000500000001pt height=26.76201000000001pt/>. This implies that they have the **same eigenvalues** too.

### Scalar product and norms in vector spaces

Let's consider a vector space <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> over the field <img src="svgs/b8bc815b5e9d5177af01fd4d3d3c2f10.svg?invert_in_darkmode" align=middle width=12.853995000000003pt height=22.46574pt/>. We define a function <img src="svgs/1547e775fb2eee42b0bd9c907a9552ed.svg?invert_in_darkmode" align=middle width=95.502pt height=24.65759999999998pt/> as a **norm** if it satisfies the following properties:

- It is always **greater or equal to <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>**, and it is <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/> **only when the vector itself is**: 
  <img src="svgs/7fc3fcfc64e8a2596640f71e2d5cf3af.svg?invert_in_darkmode" align=middle width=292.876155pt height=24.65759999999998pt/>
- If **multiplied by a constant**, the result is the norm **multiplied by the absolute value** of the constant: <img src="svgs/fd2f2e08ce2d6d466fb73f70f283b2c7.svg?invert_in_darkmode" align=middle width=223.70320499999997pt height=24.65759999999998pt/>
- The **norm of a sum** is **less or equal** than the **sum of the norms**: <img src="svgs/f8c5b943c0c3c782148b0bac5f3b6de1.svg?invert_in_darkmode" align=middle width=240.95890499999996pt height=24.65759999999998pt/>

If <img src="svgs/c87ca14efe98f13246c068238f27f47b.svg?invert_in_darkmode" align=middle width=42.53980500000001pt height=22.64855999999997pt/>, its module is called a *normed space*. 

Now, considering a generic vector <img src="svgs/6c4adbc36120d62b98deef2a20d5d303.svg?invert_in_darkmode" align=middle width=8.557890000000002pt height=14.155350000000013pt/>, its **euclidean norm** is <img src="svgs/cccf136bca22b2a24bbd86874857eeb7.svg?invert_in_darkmode" align=middle width=160.387755pt height=29.899979999999978pt/>. Its **one norm** is <img src="svgs/cbdc11177ef27fa40dfa9b200fae274d.svg?invert_in_darkmode" align=middle width=120.75162000000002pt height=26.438939999999977pt/>. Its **infinity norm** is <img src="svgs/317ff3a45415038bbfa22a3928d8875e.svg?invert_in_darkmode" align=middle width=168.192255pt height=24.65759999999998pt/>. Some of you may have noted a pattern: we can generalize the concept with the **p-norm** of <img src="svgs/6c4adbc36120d62b98deef2a20d5d303.svg?invert_in_darkmode" align=middle width=8.557890000000002pt height=14.155350000000013pt/>, which is <img src="svgs/51fb8191d3056fd200d94e54b1b7f72c.svg?invert_in_darkmode" align=middle width=249.05380499999998pt height=35.57103pt/>.

Two norms are said **equivalent** if we can find two positive constants <img src="svgs/71c4fccbd0db74aca45d5bea8971c4d3.svg?invert_in_darkmode" align=middle width=20.328165000000006pt height=14.155350000000013pt/> and <img src="svgs/646507d9bddf0d2330f6cb4a9ee4aef0.svg?invert_in_darkmode" align=middle width=24.96318pt height=22.46574pt/> that we can multiply the norm for to obtain the following inequality: <img src="svgs/16f873169a8bd657465f41c6d9ca98de.svg?invert_in_darkmode" align=middle width=244.57735499999998pt height=24.65759999999998pt/>. This is a fancy concept but it reveals a great thing: **in a vector space all the <img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/> norms are equivalent!**

### Matrix norms

A matrix norm is a function satisfying the above-mentioned properties. We can say that a matrix norm <img src="svgs/7967404d7b62656850c755d46b2a46d0.svg?invert_in_darkmode" align=middle width=28.767255000000002pt height=24.65759999999998pt/> is **compatible** with a vector norm <img src="svgs/dbbf39034141548942b5402cebc12d73.svg?invert_in_darkmode" align=middle width=25.83339pt height=24.65759999999998pt/> if the norm of the multiplication is lesser or equal than the multiplication of the norms: <img src="svgs/e8b1886971e51fb0e3045a97bafc8a2e.svg?invert_in_darkmode" align=middle width=108.15073499999997pt height=24.65759999999998pt/>.

The **spectral norm** of a matrix is equal to <img src="svgs/62843c08ef0ffb3081a46e32a45403b5.svg?invert_in_darkmode" align=middle width=133.534995pt height=29.70791999999998pt/>. Note that the **spectral norm of the identity matrix** is equal to <img src="svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>.

We can say that the ***1-norm*** of a matrix is the maximum between the sums of the **columns** absolute values: <img src="svgs/c2a28a5ba4b64e8b1903d865397e5027.svg?invert_in_darkmode" align=middle width=214.21570499999999pt height=26.438939999999977pt/>. Obviously, when dealing with identity matrices this will be equal to <img src="svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>.

The ***infinity norm*** is the same concept, but on **rows**: <img src="svgs/6c860bba4729dbe8f12b979ab76ea43c.svg?invert_in_darkmode" align=middle width=220.76950499999998pt height=26.438939999999977pt/>. Notice that on **symmetric matrices** these two values coincide. Obviously, when dealing with identity matrices this will be equal to <img src="svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>.

Lastly, we define the **Frobenius norm** of a matrix <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> as the square root of the sum of all the squares:

<img src="svgs/2c5725ab5cad719ae6a3c44e6e810040.svg?invert_in_darkmode" align=middle width=167.040555pt height=39.5802pt/>. For identity matrices, this will be equal to <img src="svgs/4fd78aba72015f7697ab298a89ec8a9c.svg?invert_in_darkmode" align=middle width=23.565630000000002pt height=24.99551999999999pt/>.

# Matrix decompositions

**Matrix factorisations** (aka decompositions) are incredibly useful tools for linear algebra problems. They write a generic matrix <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> as a product of matrices, that are usually easier to compute (for example, triangulars or diagonals).

Let's start by considering factorizations by **triangular matrices**.

<a name="lufact"></a>

## Gaussian elimination method 

Let's consider a square matrix <img src="svgs/40cc18b0e7d1320f7eb9bd9d3fd71470.svg?invert_in_darkmode" align=middle width=70.81816500000001pt height=26.177579999999978pt/>. If <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is non-singular (therefore, it is invertible) and all its **principal minors** are non-singular, then we can find two matrices <img src="svgs/ddcb483302ed36a59286424aa5e0be17.svg?invert_in_darkmode" align=middle width=11.187330000000003pt height=22.46574pt/> and <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> that, multiplied, result in <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>. The cool thing is that <img src="svgs/ddcb483302ed36a59286424aa5e0be17.svg?invert_in_darkmode" align=middle width=11.187330000000003pt height=22.46574pt/> stands for **Lower triangular**, while <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> stands for **Upper triangular**. Pretty useful, huh?

We call this the **LU factorization**.

The matrices can be computed in <img src="svgs/efcf8d472ecdd2ea56d727b5746100e3.svg?invert_in_darkmode" align=middle width=38.17737pt height=21.18732pt/> steps with the so-called **Gaussian Elimination Method**, which has a computational cost of <img src="svgs/ef9d2b7e8ee7d6389b95c3ee58562a02.svg?invert_in_darkmode" align=middle width=60.008685pt height=26.76201000000001pt/>. We can start by computing the matrix <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> as follows: we define a matrix <img src="svgs/f4125d0f27977a227268af329c65cc60.svg?invert_in_darkmode" align=middle width=39.865485pt height=29.19113999999999pt/> of multipliers, with the diagonal elements equal to <img src="svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>, the elements under the diagonal **on the column k** <img src="svgs/51b305325a6d990c19d75c17818c3f52.svg?invert_in_darkmode" align=middle width=207.709755pt height=42.51654pt/>, and <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/> otherwise. We then compute <img src="svgs/db29c54bcab39a393b805f0f4d7fd681.svg?invert_in_darkmode" align=middle width=135.222945pt height=29.19113999999999pt/>, then iterate until we obtain <img src="svgs/0141c6862df0df028b327d2313bec2ec.svg?invert_in_darkmode" align=middle width=66.48444pt height=29.19113999999999pt/>.

There's just one problem: this algorithm is **unstable**! This means that the **algorithmic error is not limited**, and it happens because the elements <img src="svgs/2dcd38efa74bb3fe4a77f85e7b661451.svg?invert_in_darkmode" align=middle width=26.229225000000003pt height=34.33782pt/> can be ultra small (or even zero!), thus leading to errors. We can solve this problem by computing the **pivoting algorithm**, i.e. we swap two rows so that <img src="svgs/2dcd38efa74bb3fe4a77f85e7b661451.svg?invert_in_darkmode" align=middle width=26.229225000000003pt height=34.33782pt/> is the element with the maximum absolute value. We then get a **permutation matrix** <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.836835000000004pt height=22.46574pt/>, i.e. an **identity matrix with the needed rows swapped**.

When we're dealing with **symmetric positive definite** matrices there's a great simplification: it is always possible to compute the LU factorization **without pivoting** and it simply is <img src="svgs/22c698b7ad423606bd4f040e74c676c9.svg?invert_in_darkmode" align=middle width=66.15477pt height=27.656969999999987pt/>. This factorization is obviously less costly: using the Cholesky algorithm, it has a complexity of <img src="svgs/2b72192b16b235f27d3b5565f77fb346.svg?invert_in_darkmode" align=middle width=60.008685pt height=26.76201000000001pt/>.

We can therefore use the **Cholesky decomposition**, which is greatly used in ML because we often deal with symmetric positive definite matrices, like the covariance matrix of a multivariate Gaussian. This decomposition can be used to efficiently compute the determinant too: since the obtained matrices are triangular, the determinant will just be the **product of the diagonal**. 

## Decompositions involving diagonal matrices

These ones are about a diagonal matrix, not a triangular one.

There are two main decompositions involving diagonal matrices: the **eigendecomposition** and the **Singular Value Decomposition**. 

### Eigendecomposition

Let's consider a **square matrix** <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> of size <img src="svgs/2be744f3276b5219af5f8dd5f793e02c.svg?invert_in_darkmode" align=middle width=39.82506pt height=19.178279999999994pt/>, which can be decomposed as <img src="svgs/712dd58fc85263346c5640e112596cdf.svg?invert_in_darkmode" align=middle width=90.812865pt height=26.76201000000001pt/>. <img src="svgs/df5a289587a2f0247a5b97c1e8ac58ca.svg?invert_in_darkmode" align=middle width=12.836835000000004pt height=22.46574pt/> is a non-singular (obviously: we have to invert it!) matrix of size <img src="svgs/2be744f3276b5219af5f8dd5f793e02c.svg?invert_in_darkmode" align=middle width=39.82506pt height=19.178279999999994pt/> and <img src="svgs/78ec2b7008296ce0561cf83393cb746d.svg?invert_in_darkmode" align=middle width=14.066250000000002pt height=22.46574pt/> is a diagonal matrix with the eigenvalues on the diagonal. Beware: the eigenvectors **have to be linearly independent** and **form a basis** of <img src="svgs/8a86f4a11e2fbfc03de61d587ba826de.svg?invert_in_darkmode" align=middle width=19.998330000000006pt height=22.64855999999997pt/>. This decomposition can **only be applied to square matrices with particular properties on their spectrum**.

### Singular Value Decomposition (SVD)

This one can be applied to **all the matrices**. Pretty cool, huh? First, tho, we need to clarify a few concepts:

- Two vectors <img src="svgs/2aec3383d84485daa90d961611e9b21c.svg?invert_in_darkmode" align=middle width=65.36343000000001pt height=22.64855999999997pt/> are **orthogonal** if their product is equal to 0. This is the vector product, i.e. the product you'd obtain by multiplicating <img src="svgs/7e21464f860cf1a61bf68c7631ac42c4.svg?invert_in_darkmode" align=middle width=47.593095000000005pt height=27.656969999999987pt/>;
- A **unit vector** is a vector <img src="svgs/cdbb034375cae1a8fed3043570f71626.svg?invert_in_darkmode" align=middle width=49.499669999999995pt height=22.64855999999997pt/> which has norm equal to 1: <img src="svgs/edfdcf5e41e1b2a2b7fd9780226dc9c1.svg?invert_in_darkmode" align=middle width=55.985655pt height=24.65759999999998pt/>;
- The **normalization** of a vector is the division between the vector and its norm, which always returns a **unit vector**: <img src="svgs/2ec7537cb715af18f3297a730e6a9a39.svg?invert_in_darkmode" align=middle width=54.543060000000004pt height=22.853489999999976pt/>;
- A set of vectors <img src="svgs/d85cb90adfb500a1629eebbfedf80fcc.svg?invert_in_darkmode" align=middle width=164.91040500000003pt height=24.65759999999998pt/> is an **orthogonal set** if all of the possible products are equal to <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>, i.e. <img src="svgs/f93f967dafa9653d9e3a8aa8b8f907ea.svg?invert_in_darkmode" align=middle width=195.565755pt height=27.656969999999987pt/>;
- If some vectors are **orthogonal**, they are **linearly independent**. Therefore, the set they compose forms an orthogonal basis for <img src="svgs/6795a49b4f1add902293d6612f119dba.svg?invert_in_darkmode" align=middle width=172.09780500000002pt height=24.65759999999998pt/>;
- The set <img src="svgs/d85cb90adfb500a1629eebbfedf80fcc.svg?invert_in_darkmode" align=middle width=164.91040500000003pt height=24.65759999999998pt/> is an **orthonormal set** if it is an **orthogonal** set of **unit vectors**. A basis of orthonormal vector is called an **orthonormal basis**;
- Applying these concepts to matrices, we get that <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> is an **orthogonal matrix** iff <img src="svgs/1e67b013a99745f085f2343f746f48a9.svg?invert_in_darkmode" align=middle width=66.821205pt height=27.656969999999987pt/>. If the matrix is **square**, then <img src="svgs/5a8a83bc4970a9b00b29bf2af903094c.svg?invert_in_darkmode" align=middle width=75.13176pt height=27.656969999999987pt/>;
- If a matrix is orthogonal, then:
  - The 2-norm of the **matrix multiplied by a vector** is equal to the **2-norm of the vector only**: <img src="svgs/9c87bb374d60ff6beb5dec18445b3761.svg?invert_in_darkmode" align=middle width=165.728805pt height=24.65759999999998pt/>
  - The **product between the matrix multiplied by two vectors** is the **product of the two vectors**: <img src="svgs/f1c3170fb33dd5b7701fa4df82ff5967.svg?invert_in_darkmode" align=middle width=208.60570499999997pt height=24.65759999999998pt/>
  - Therefore, the above mentioned product is equal to <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/> **only when the product between <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> and <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/> is**: <img src="svgs/4384f2295cba71369e36f02d573d6c6a.svg?invert_in_darkmode" align=middle width=286.229955pt height=24.65759999999998pt/>
  - We can then conclude that **transformations by orthogonal matrices preserve both length and angles!**

Now we're ready for the real thing!

Any matrix <img src="svgs/ad73601e39ef9a2d0bc591935b918895.svg?invert_in_darkmode" align=middle width=74.357085pt height=26.177579999999978pt/> with <img src="svgs/20ce14c761900230f78ed61012621c14.svg?invert_in_darkmode" align=middle width=91.61163pt height=24.65759999999998pt/>, where <img src="svgs/aecf2a0ee257e027bea1c3890c510e73.svg?invert_in_darkmode" align=middle width=40.85994pt height=22.831379999999992pt/> can be written as:

<img src="svgs/cd73f89b468a51e9644a57ae2c50d085.svg?invert_in_darkmode" align=middle width=81.91029pt height=27.656969999999987pt/>

where <img src="svgs/cd0e3808823fef5f9645350c98ac2029.svg?invert_in_darkmode" align=middle width=78.58306499999999pt height=26.177579999999978pt/> is an **orthogonal matrix** with orthogonal vectors <img src="svgs/194516c014804d683d1ab5a74f8c5647.svg?invert_in_darkmode" align=middle width=14.061300000000003pt height=14.155350000000013pt/>, <img src="svgs/eb0168f11a121ecb0ef5892c9c960d1e.svg?invert_in_darkmode" align=middle width=71.73144pt height=26.177579999999978pt/> is an orthogonal matrix with orthogonal vectors <img src="svgs/9f7365802167fff585175c1750674d42.svg?invert_in_darkmode" align=middle width=12.619035000000006pt height=14.155350000000013pt/>, and <img src="svgs/a4dd428e653a866fe122b5c285e8b60e.svg?invert_in_darkmode" align=middle width=73.90053pt height=26.177579999999978pt/> is a matrix whose diagonal entries are the **singular values** <img src="svgs/e61ae7f2cb94c8418c30517775fde77d.svg?invert_in_darkmode" align=middle width=14.044140000000004pt height=14.155350000000013pt/> of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> and with extra-diagonal entries equal to <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>. <img src="https://cdn.mathpix.com/snip/images/N3oanb3v-5R-_eXPha80gAU0FPOP4Y7gUgHKkjIDZrM.original.fullsize.png" />

The singular values are in **ascending order** in the diagonal, and the first <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/> are non-null while the other <img src="svgs/47c4614e970adb63a68a4037abbb66ad.svg?invert_in_darkmode" align=middle width=39.03355500000001pt height=22.831379999999992pt/> are equal to <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>. The singular matrix <img src="svgs/813cd865c037c89fcdc609b25c465a05.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.46574pt/> **is unique**, the other two **aren't**.

Observe that the matrix <img src="svgs/813cd865c037c89fcdc609b25c465a05.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.46574pt/> is rectangular, this means that it has a diagonal submatrix that contains the singular values and needs additional zero padding. 

From the geometric point of view, the SVD is nothing more than sequential linear transformations performed on the bases. The SVD intuition is similar to the eigendecomposition one: broadly speaking, it performs a basis change via <img src="svgs/4cb4714074eaef8a432de7a7f9594820.svg?invert_in_darkmode" align=middle width=22.775775000000003pt height=27.656969999999987pt/>, followed by a scaling and augmentation/reduction in dimensionality via the singular value matrix <img src="svgs/813cd865c037c89fcdc609b25c465a05.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.46574pt/>. Finally, it performs a second basis change via <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/>.

Basically: 

- <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> performs a basis change in the domain <img src="svgs/8a86f4a11e2fbfc03de61d587ba826de.svg?invert_in_darkmode" align=middle width=19.998330000000006pt height=22.64855999999997pt/> from <img src="svgs/3a4b11477082188db06b7b3af9da3666.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=30.267599999999987pt/> to the standard basis <img src="svgs/61e84f854bc6258d4108d08d4c4a0852.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=22.46574pt/>. The inverse of <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> (which, as <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> is orthogonal, is <img src="svgs/2229d330ef74da68bf0b50eba6d39de2.svg?invert_in_darkmode" align=middle width=75.58386pt height=27.656969999999987pt/>) performs the inverse change from <img src="svgs/61e84f854bc6258d4108d08d4c4a0852.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=22.46574pt/> to <img src="svgs/3a4b11477082188db06b7b3af9da3666.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=30.267599999999987pt/>;
- Having changed the coordinate system to <img src="svgs/e1c304a11300e11f1b5826600eb8bc95.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=30.267599999999987pt/>, we can scale to the new coordinates by the matrix <img src="svgs/813cd865c037c89fcdc609b25c465a05.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.46574pt/>, i.e. by the singular values <img src="svgs/e61ae7f2cb94c8418c30517775fde77d.svg?invert_in_darkmode" align=middle width=14.044140000000004pt height=14.155350000000013pt/>;
- Finally, <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> performs a basis change in the codomain <img src="svgs/1281caf41453d6d5cb92c8276ef582dd.svg?invert_in_darkmode" align=middle width=23.537085000000005pt height=22.64855999999997pt/> from <img src="svgs/565c7b3b11e4bd7f28c691e0249b88e7.svg?invert_in_darkmode" align=middle width=12.924780000000005pt height=30.267599999999987pt/> to the canonical basis of <img src="svgs/1281caf41453d6d5cb92c8276ef582dd.svg?invert_in_darkmode" align=middle width=23.537085000000005pt height=22.64855999999997pt/>.

Summing things up, SVD performs **two changes of basis**, via the orthogonal matrices <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> and <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> and a scaling operation via the matrix <img src="svgs/813cd865c037c89fcdc609b25c465a05.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.46574pt/>. The columns of <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> and <img src="svgs/a9a3a4a202d80326bda413b5562d5cd1.svg?invert_in_darkmode" align=middle width=13.242075000000003pt height=22.46574pt/> are orthonormal basis of <img src="svgs/1281caf41453d6d5cb92c8276ef582dd.svg?invert_in_darkmode" align=middle width=23.537085000000005pt height=22.64855999999997pt/> and <img src="svgs/8a86f4a11e2fbfc03de61d587ba826de.svg?invert_in_darkmode" align=middle width=19.998330000000006pt height=22.64855999999997pt/>, respectively. The change of basis is in both the domain and the codomain: this is in contrast with the eigendecomposition, which operates within the same vector space (the basis change is applied but then undone). What makes SVD special is that these two different bases are simultaneously linked by the singular value matrix <img src="svgs/813cd865c037c89fcdc609b25c465a05.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.46574pt/>.

We can link the singular values of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> to the eigenvalues of <img src="svgs/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode" align=middle width=35.01333pt height=27.656969999999987pt/>: 

- Let's first substitute A by its SVD: <img src="svgs/f435c2f9e43e1a11523a65076941ef1f.svg?invert_in_darkmode" align=middle width=318.25150499999995pt height=27.656969999999987pt/>, which, since <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> is orthogonal, simplifies to <img src="svgs/5949b2245e2c31e0ab386180ed8f8227.svg?invert_in_darkmode" align=middle width=78.63372pt height=27.656969999999987pt/>;
- We know that <img src="svgs/b664993acd5987e42763079d8c9225b8.svg?invert_in_darkmode" align=middle width=34.100055000000005pt height=27.656969999999987pt/> simplifies to a diagonal matrix with the singular values (squared) in it: <img src="svgs/c494c40ba8272afb0e3b9f07c523e989.svg?invert_in_darkmode" align=middle width=336.24475499999994pt height=77.48333999999998pt/> 

Hence, we know that <img src="svgs/798ff1b69d81778edee4e8e38513268f.svg?invert_in_darkmode" align=middle width=190.059705pt height=29.70791999999998pt/>. In particular, the **first** one will be <img src="svgs/0be7defadd529f2d21f723689100300e.svg?invert_in_darkmode" align=middle width=196.81975500000001pt height=29.70791999999998pt/>, and the **last** one will be <img src="svgs/8ae61d790ab08265a135ddb813ff280e.svg?invert_in_darkmode" align=middle width=256.704855pt height=29.70791999999998pt/>, so the 2-norm of the inverse will be the inverse of <img src="svgs/28cf960b1f96e750df70968130f6b0db.svg?invert_in_darkmode" align=middle width=17.519205000000003pt height=14.155350000000013pt/>.

Therefore, the left singular vectors of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> are the eigenvectors of <img src="svgs/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode" align=middle width=35.01333pt height=27.656969999999987pt/>. The right singular vectors of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> are the eigenvectors of <img src="svgs/d4f5cd9596d7bf6138dc0de701a2ffc1.svg?invert_in_darkmode" align=middle width=34.1913pt height=27.656969999999987pt/>. **For symmetric matrices, the eigendecomposition and the SVD are the same thing.**

### Moore-Penrose inverse

The SVD can be used to compute a pseudo-inverse, named **Moore-Penrose inverse** of a matrix <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>:

<img src="svgs/1cae54ed1cdb64708c40c6df487da859.svg?invert_in_darkmode" align=middle width=103.73698499999999pt height=27.656969999999987pt/>, where <img src="svgs/dc13ef5e6a0d7b25eb96e800fe53b9b2.svg?invert_in_darkmode" align=middle width=21.963645000000003pt height=26.177579999999978pt/> is the pseudoinverse of <img src="svgs/813cd865c037c89fcdc609b25c465a05.svg?invert_in_darkmode" align=middle width=11.872245000000005pt height=22.46574pt/>, which is computable by taking the reciprocal of every non-zero diagonal element and transposing the matrix:

<img src="svgs/7635e2e138b0c84e1830649992af913a.svg?invert_in_darkmode" align=middle width=461.74870500000003pt height=181.39076999999997pt/>

### Matrix approximation using SVD

Given the SVD of a matrix <img src="svgs/8bfe675241e89fad660320923a91a157.svg?invert_in_darkmode" align=middle width=47.66388pt height=27.656969999999987pt/>, we can use it to represent the matrix <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> as a **sum of low-rank matrices** <img src="svgs/ba15937b4998e8d17369c8df88ccaf91.svg?invert_in_darkmode" align=middle width=79.829805pt height=26.177579999999978pt/> with <img src="svgs/eb71a14b5c38e753b886f895f9e1256f.svg?invert_in_darkmode" align=middle width=96.22833pt height=24.65759999999998pt/> such that <img src="svgs/73928680f8ee97c30f84bd7fd7d080d2.svg?invert_in_darkmode" align=middle width=72.69388500000001pt height=27.656969999999987pt/>. The matrix <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> can then be written as the **sum** (to <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/>, which is the original rank of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>) of **these matrices multiplied by the singular values**: <img src="svgs/c6d1565538b10b98bf4e3d9297a208d3.svg?invert_in_darkmode" align=middle width=221.08795499999997pt height=32.51192999999998pt/>. To obtain a rank-<img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/> approximation of the matrix, we can **truncate the sum** at the index <img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/>. The error introduced with this approximation can be computed as the **2-norm of the difference**, <img src="svgs/acc3d1c804f7fb9c000092508a1c1b00.svg?invert_in_darkmode" align=middle width=75.33817499999999pt height=24.65759999999998pt/>, which is equal to the **sum of the matrices from <img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/> to <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/>:** this quantity, though, is simply the **next singular value** <img src="svgs/2fb0728c73c8bc07384ac627b2476fbd.svg?invert_in_darkmode" align=middle width=32.813550000000006pt height=14.155350000000013pt/>. Great! This means that if <img src="svgs/2fb0728c73c8bc07384ac627b2476fbd.svg?invert_in_darkmode" align=middle width=32.813550000000006pt height=14.155350000000013pt/> is small we have a good approximation and we can stop.

Now, if we have a rank-<img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/> approximation <img src="svgs/7555b00e2f6df9ab19ad9738f9c91b7a.svg?invert_in_darkmode" align=middle width=131.89341000000002pt height=27.656969999999987pt/>, for every matrix of the same rank <img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/>, we get that the 2-norm of the difference will always be greater or equal than the difference from <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> to <img src="svgs/f7e5e35d144ef377e6586f25a5c7e554.svg?invert_in_darkmode" align=middle width=19.105350000000005pt height=22.46574pt/>. This is just a fancy way of saying that <img src="svgs/f7e5e35d144ef377e6586f25a5c7e554.svg?invert_in_darkmode" align=middle width=19.105350000000005pt height=22.46574pt/> is the best way of approximating <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> with a rank <img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/>. No other <img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/>-rank matrix does that better: <img src="svgs/0c5a1962217bbb779f5377f370042989.svg?invert_in_darkmode" align=middle width=350.45455499999997pt height=26.177579999999978pt/>.

Note that this is a lossy compression. 

### Let's sum up things!

We can therefore conclude that we can decompose a matrix in lots of ways:

![Matrix decompositions](./res/matrix-decompositions.png)





# Linear systems

A **linear system** can be written as <img src="svgs/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode" align=middle width=50.69625pt height=22.831379999999992pt/>, where <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is a matrix of size <img src="svgs/63b142315f480db0b3ff453d62cc3e7f.svg?invert_in_darkmode" align=middle width=44.391270000000006pt height=19.178279999999994pt/> (let's suppose <img src="svgs/0964aa9a8c5b7a612534543d20ddc673.svg?invert_in_darkmode" align=middle width=46.217655pt height=20.908799999999992pt/> for now),  <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> is a column vector of length <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> and <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/> is a column vector of length <img src="svgs/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode" align=middle width=14.433210000000003pt height=14.155350000000013pt/>. <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> represents the unknown solution, while <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/> is given. This form can therefore be expanded as 

<img src="svgs/68b6432b2fc96d39d647d77e05e7c5c8.svg?invert_in_darkmode" align=middle width=256.72911000000005pt height=93.27878999999997pt/>

We are interested in some informations about the system:

- Does a solution **exist**? Is it **unique**?
- What are the **numerical methods** we can exploit to actually find this solution?
- What are the **conditions** of the problem?

Let's separately consider the case of **square** linear systems and **least square** ones.

## Square linear systems

The solution of the system <img src="svgs/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode" align=middle width=50.69625pt height=22.831379999999992pt/> where A has size <img src="svgs/2be744f3276b5219af5f8dd5f793e02c.svg?invert_in_darkmode" align=middle width=39.82506pt height=19.178279999999994pt/> and <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/> has size <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> **exists and is unique** if and only if one of the following conditions is true:

- A is **non-singular**;
- The rank of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/>: <img src="svgs/c2ad9b3735be33abe87fd6a76c20de5f.svg?invert_in_darkmode" align=middle width=92.40313499999999pt height=24.65759999999998pt/>;
- The system only admits the solution <img src="svgs/8436d02a042a1eec745015a5801fc1a0.svg?invert_in_darkmode" align=middle width=39.53185500000001pt height=21.18732pt/>.

The solution can be algebrically computed by calculation of the inverse of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>: <img src="svgs/f02a9fe447e5053233378f67fcd0f12f.svg?invert_in_darkmode" align=middle width=144.61161pt height=26.76201000000001pt/>. The problem is that computing the inverse can be pretty difficult. We can exploit the **Cramer's rule**, which simply allows us to calculate the <img src="svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?invert_in_darkmode" align=middle width=14.045955000000003pt height=14.155350000000013pt/>s by substituting the <img src="svgs/77a3b857d53fb44e33b53e4c8b68351a.svg?invert_in_darkmode" align=middle width=5.663295000000005pt height=21.683310000000006pt/>-th column of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> with <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/>, obtaining <img src="svgs/4ebf880807deff5796460f39aea46f80.svg?invert_in_darkmode" align=middle width=16.979820000000004pt height=22.46574pt/>, then dividing the determinant of <img src="svgs/4ebf880807deff5796460f39aea46f80.svg?invert_in_darkmode" align=middle width=16.979820000000004pt height=22.46574pt/> for the determinant of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>: <img src="svgs/4722257d2e513af80075e7e2c0276bc9.svg?invert_in_darkmode" align=middle width=165.81295500000002pt height=29.461410000000004pt/>.

The computation can still be pretty complicated, so we can distinguish between two methodologies:

- **Direct methods** yield the solution in a finite number of steps, which are usually computationally costly;
- **Iterative methods** theoretically require an infinite number of steps: they converge to the solution for <img src="svgs/7127d8b7d6f62743e98c1fe712bd4191.svg?invert_in_darkmode" align=middle width=47.672295000000005pt height=21.683310000000006pt/>. They are less precise but less expensive.

It is also very important to check the errors we obtained, that are usually generated in two ways: the **rounding errors** (aka arithmetic errors), which depend on the algorithm steps, and **inherent errors**, which depend on the data representation and not on the algorithm. When the first ones are limited to a constant, we can define the algorithm **stable**. When the latter are large, we define the problem as **ill-posed**.

### Direct methods

To solve the problem, we can [LU factorize the matrix](#lufact), then solve two triangular systems: <img src="svgs/83d259ea7b38ad807b9299e13981fcd3.svg?invert_in_darkmode" align=middle width=109.092555pt height=22.831379999999992pt/>. This can be obtained by: <img src="svgs/c4254968840ed022c69d56ef9ea23542.svg?invert_in_darkmode" align=middle width=380.85085499999997pt height=26.76201000000001pt/>.

If we're using the pivoting algorithm, we have to remind that <img src="svgs/549ff8d7e94442ee4eea3366cfffeb7f.svg?invert_in_darkmode" align=middle width=76.369755pt height=22.831379999999992pt/>, therefore <img src="svgs/1c728a3e964dc62f37c4faf87f41e9b5.svg?invert_in_darkmode" align=middle width=121.92939pt height=22.831379999999992pt/>.

### Iterative methods

**Iterative methods** allow us to approximate the solution, which ideally would be obtained with an infinite amount of steps. 

The basic idea is to construct a sequence of vectors <img src="svgs/41a0912d0f46af38c7fa2115d8f0386e.svg?invert_in_darkmode" align=middle width=16.661040000000003pt height=14.155350000000013pt/> that converge to the solution: <img src="svgs/572fbb6188fe06833df3ce10d619f697.svg?invert_in_darkmode" align=middle width=115.15349999999998pt height=22.831379999999992pt/>, where <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/> is the exact solution and the starting guess <img src="svgs/e714a3139958da04b41e3e607a544455.svg?invert_in_darkmode" align=middle width=15.947580000000002pt height=14.155350000000013pt/> is given. 

In general, the sequence <img src="svgs/41a0912d0f46af38c7fa2115d8f0386e.svg?invert_in_darkmode" align=middle width=16.661040000000003pt height=14.155350000000013pt/> is obtained through a function (or a particular set of operations) <img src="svgs/3cf4fbd05970446973fc3d9fa3fe3c41.svg?invert_in_darkmode" align=middle width=8.430510000000004pt height=14.155350000000013pt/>, that acts on the last <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>: <img src="svgs/de747edd5f9f4b421421aa0b937c6845.svg?invert_in_darkmode" align=middle width=94.92581999999999pt height=24.65759999999998pt/>. Different classes of iterative methods are available, with the most common being **stationary iterative methods** and **gradient-like methods**. The first ones take the form <img src="svgs/2ff51da76a00b2d0156371305da96a2e.svg?invert_in_darkmode" align=middle width=116.729415pt height=22.831379999999992pt/>, where <img src="svgs/61e84f854bc6258d4108d08d4c4a0852.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=22.46574pt/> is called an *iteration matrix* and <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is a vector obtained from <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/>. The latter take the form of <img src="svgs/fd9600b0ea778ea3a101edeaf5a56f46.svg?invert_in_darkmode" align=middle width=127.75883999999998pt height=19.178279999999994pt/>, where <img src="svgs/520ccf06899de2fec066cf3ff72514a5.svg?invert_in_darkmode" align=middle width=50.566889999999994pt height=22.64855999999997pt/> is a constant called **stepsize** and <img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/> is a vector called **direction.**

If <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is symmetric and positive definite, and the vectors <img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/> have the conjugacy property (which basically means that <img src="svgs/d21ad60891c8cb92a3006d4bb499ed9c.svg?invert_in_darkmode" align=middle width=136.89307499999998pt height=27.656969999999987pt/>) the method is called **conjugate gradients**.

All of these methods require a matrix multiplication for every step: therefore, the computational complexity is <img src="svgs/c5566036dd2bd924fef1c6263072eb45.svg?invert_in_darkmode" align=middle width=43.57023pt height=26.76201000000001pt/>. Since the steps should be infinite, we have to define stopping criteria, which are usually based on the residual <img src="svgs/5e0fb508b2d36e5b54723e62fb2376f8.svg?invert_in_darkmode" align=middle width=93.557805pt height=22.831379999999992pt/>, in the absolute way (<img src="svgs/32e2db224f97440bb4832398e6d5a1f8.svg?invert_in_darkmode" align=middle width=60.532725pt height=24.65759999999998pt/>) or relative (<img src="svgs/2236d05875f5877ef69913f4f393d83f.svg?invert_in_darkmode" align=middle width=57.322649999999996pt height=33.20559pt/>). We can even just check how much the <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> changes from one iteration to the other: <img src="svgs/e9c2aa7b68d816d2a8c6baaf565dfe41.svg?invert_in_darkmode" align=middle width=119.10393pt height=24.65759999999998pt/>.

### Inherent errors in linear systems

Remember that **inherent errors** only depend on the data representation, not on the algorithm.

We should therefore try to consider **what happens to the solution** <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> when the input data **slightly change**. Let's suppose that <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> doesn't change, and <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/> changes by a quantity <img src="svgs/0d03c7a2c2508c590e57358de56f07df.svg?invert_in_darkmode" align=middle width=20.753535000000003pt height=22.831379999999992pt/>: <img src="svgs/6ad85c3017c6799e1e6b9e86fcb09e61.svg?invert_in_darkmode" align=middle width=147.511155pt height=24.65759999999998pt/>. We now want to **compare the relative changes** <img src="svgs/98dfe091fb378cd8c904eca107690a28.svg?invert_in_darkmode" align=middle width=40.46361pt height=28.67072999999997pt/> and <img src="svgs/7db388610bebbbd75040d74bad88ed26.svg?invert_in_darkmode" align=middle width=38.79018000000001pt height=28.926479999999973pt/> to see how much the solution changes. Let's subtract <img src="svgs/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode" align=middle width=50.69625pt height=22.831379999999992pt/> to the equation we obtained: <img src="svgs/ec74bb1b76ac6331dadf3f76fd321c21.svg?invert_in_darkmode" align=middle width=78.093675pt height=22.831379999999992pt/>, then extract <img src="svgs/3919bbc84b8079e27194efe99a1f6a80.svg?invert_in_darkmode" align=middle width=23.09373pt height=22.46574pt/>: <img src="svgs/cdbe937f3513c2f4eff52d28d7a0bae4.svg?invert_in_darkmode" align=middle width=95.742075pt height=26.76201000000001pt/>. We know as a property of norms that the norm of the multiplication is always lesser or equal than the multiplication of the norms: <img src="svgs/74682a6f23fa2391f2aaafa14ccd5483.svg?invert_in_darkmode" align=middle width=172.695105pt height=26.76201000000001pt/>.

Getting back to our usual equation, we know that <img src="svgs/aeb0df286b1708ef52b568fce4e3cb0d.svg?invert_in_darkmode" align=middle width=83.57316pt height=24.65759999999998pt/>, mirror it, <img src="svgs/0dd43818682484db6c6c6cdf16145ec5.svg?invert_in_darkmode" align=middle width=83.57316pt height=24.65759999999998pt/>, then observe that, as before, the norm of the multiplication is lesser or equal than the multiplication of the norms: <img src="svgs/27c0c049e6cf2d50ec09a7feefcac113.svg?invert_in_darkmode" align=middle width=114.68044499999999pt height=24.65759999999998pt/>. We can therefore conclude that <img src="svgs/7cf26ad1eb4aa9e65f91d29e968acab2.svg?invert_in_darkmode" align=middle width=79.84712999999999pt height=28.926479999999973pt/>.

Let's put together all of these things and we get that 

<img src="svgs/b3c1b2b1ded522859853a704bded2424.svg?invert_in_darkmode" align=middle width=193.331655pt height=33.20559pt/>. We can give a name to <img src="svgs/46dd5742a919e73b90b59f595dd5d5c4.svg?invert_in_darkmode" align=middle width=87.05499pt height=26.76201000000001pt/>, and we'll call it **condition number** <img src="svgs/0044dd7421e9e5ad6989f7ed5a24b3e1.svg?invert_in_darkmode" align=middle width=40.25125500000001pt height=24.65759999999998pt/>.

In general, <img src="svgs/0044dd7421e9e5ad6989f7ed5a24b3e1.svg?invert_in_darkmode" align=middle width=40.25125500000001pt height=24.65759999999998pt/> depends on the choice of the norm, indicated by a subscript. Notice that <img src="svgs/efe1423de3a871920d3e2bce8e416949.svg?invert_in_darkmode" align=middle width=70.388175pt height=24.65759999999998pt/> since <img src="svgs/c4add658cdb19a010a2775808433b25d.svg?invert_in_darkmode" align=middle width=72.73967249999998pt height=26.76175259999998pt/>, therefore has norm equal to 1,

 but basing on the norm properties we know that the norm of <img src="svgs/12006f890b98ff5e1b3c0e226ff98bde.svg?invert_in_darkmode" align=middle width=41.484300000000005pt height=26.76201000000001pt/> will always be greater or equal than the single norms multiplied. If <img src="svgs/0044dd7421e9e5ad6989f7ed5a24b3e1.svg?invert_in_darkmode" align=middle width=40.25125500000001pt height=24.65759999999998pt/> is large, we know that the matrix <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is almost a singular matrix and its column are almost linearly dependent. Regularization techniques can reduce <img src="svgs/0044dd7421e9e5ad6989f7ed5a24b3e1.svg?invert_in_darkmode" align=middle width=40.25125500000001pt height=24.65759999999998pt/>. 

## Linear least squares

Now, let's consider an *overdetermined* system <img src="svgs/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode" align=middle width=50.69625pt height=22.831379999999992pt/>, where <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> has dimension <img src="svgs/205995f88b807b2f5268f7ef4053f049.svg?invert_in_darkmode" align=middle width=44.391270000000006pt height=19.178279999999994pt/> with <img src="svgs/079bfec7814e7f4bcbae5a5e2830bb51.svg?invert_in_darkmode" align=middle width=46.217655pt height=17.723969999999973pt/>, <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> has dimension <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> and <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/> has dimension <img src="svgs/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode" align=middle width=14.433210000000003pt height=14.155350000000013pt/>.

Such a system usually **has no solution**: <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/> does not lie on the subspace spanned by the columns of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>. Since no solution exists, we'd like to get an **approximation**.

We'll need some tools first.

**Projections** are an important class of linear transformations. When working in ML, we often have high-dimensional data which is pretty hard to visualize. Oftentimes, only a few dimensions are useful: we can therefore compress the data to still get *knowledge* from a reduced set of data, minimizing the loss.

The **projection** <img src="svgs/10589ca5ac24e2cba75f0e75288ab2e0.svg?invert_in_darkmode" align=middle width=79.00167pt height=24.65792999999999pt/> is the closest point to <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> of the subspace <img src="svgs/6bac6ec50c01592407695ef84f457232.svg?invert_in_darkmode" align=middle width=13.016025000000003pt height=22.46574pt/> generated by <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/>. With *closest*, we mean that it has the least distance of the 2-norm. The vector <img src="svgs/8ccabcaaef27d56f82110c9d6eb01295.svg?invert_in_darkmode" align=middle width=37.853805pt height=19.178279999999994pt/> is orthogonal to the subspace U, which just means that <img src="svgs/1ea7a32e1bd757170fca6bcc157cd9d8.svg?invert_in_darkmode" align=middle width=99.200475pt height=24.65759999999998pt/>.

The vector <img src="svgs/f93ce33e511096ed626b4719d50f17d2.svg?invert_in_darkmode" align=middle width=8.367645000000003pt height=14.155350000000013pt/> is a vector of the subspace generated by <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/>, hence <img src="svgs/531fb18062e89cb9c9d5f48f930473d5.svg?invert_in_darkmode" align=middle width=97.76233500000001pt height=22.831379999999992pt/>.

We can find <img src="svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width=10.576500000000003pt height=14.155350000000013pt/> by knowing that <img src="svgs/5c79c7bff804ad14566f2bffe7a1926b.svg?invert_in_darkmode" align=middle width=386.27770499999997pt height=24.65759999999998pt/>. 

Let's divide and we get <img src="svgs/429cd8e6433705dc859e6e75f5b0d0c6.svg?invert_in_darkmode" align=middle width=113.77492499999998pt height=34.09889999999999pt/>.

### Best approximation theorem

Let's consider a subspace <img src="svgs/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode" align=middle width=17.808285000000005pt height=22.46574pt/>, with dimension <img src="svgs/2ec6e630f199f589a2402fdf3e0289d5.svg?invert_in_darkmode" align=middle width=8.270625000000004pt height=14.155350000000013pt/> of <img src="svgs/cc133b3cfa2e80f3f3f643e382cf155f.svg?invert_in_darkmode" align=middle width=76.86458999999999pt height=22.64855999999997pt/> and <img src="svgs/fae516a44f8e4b8772ef55918b2ebc98.svg?invert_in_darkmode" align=middle width=97.83691499999999pt height=24.65759999999998pt/>.

Then, the distance <img src="svgs/8d3152950d55654b2fa7ecfb7df4a551.svg?invert_in_darkmode" align=middle width=236.76625499999997pt height=24.65759999999998pt/>. This means that <img src="svgs/282f38ecf82d8d7b9d2813044262d5f3.svg?invert_in_darkmode" align=middle width=9.347580000000002pt height=22.831379999999992pt/> is the **best approximation** of <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/> in the subspace <img src="svgs/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode" align=middle width=17.808285000000005pt height=22.46574pt/>, i.e. no other vector in the subspace is as near as <img src="svgs/282f38ecf82d8d7b9d2813044262d5f3.svg?invert_in_darkmode" align=middle width=9.347580000000002pt height=22.831379999999992pt/>.

The idea is to use projections to find the vector in <img src="svgs/0e53e107f05a8643bcdb471fb09536b8.svg?invert_in_darkmode" align=middle width=51.235305000000004pt height=24.65759999999998pt/>, the subspace generated by set of all the columns of A, which is closest to <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/>. As we have just seen, this vector is nothing more than the orthogonal projection of <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/> onto the subspace <img src="svgs/0e53e107f05a8643bcdb471fb09536b8.svg?invert_in_darkmode" align=middle width=51.235305000000004pt height=24.65759999999998pt/>.

### The linear least squares problem

As already said, finding the solution of <img src="svgs/6ffa573707fca115cad7b243d91a7109.svg?invert_in_darkmode" align=middle width=50.69625pt height=22.831379999999992pt/> when <img src="svgs/079bfec7814e7f4bcbae5a5e2830bb51.svg?invert_in_darkmode" align=middle width=46.217655pt height=17.723969999999973pt/> is impossible. The orthogonal projections allow us to find a vector <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> that is **near** (i.e. has a low 2-norm distance) the solution. The problem is then formulated as to find a <img src="svgs/f84e86b97e20e45cc17d297dc794b3e8.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=22.831379999999992pt/> that minimises the distance: <img src="svgs/639d1fcebf524a510cb19cd4dbc797f3.svg?invert_in_darkmode" align=middle width=166.40530499999997pt height=26.76201000000001pt/>.

To analyze the existence and uniqueness of the solution, we can consider two different cases:

- <img src="svgs/c2ad9b3735be33abe87fd6a76c20de5f.svg?invert_in_darkmode" align=middle width=92.40313499999999pt height=24.65759999999998pt/>, meaning that every column of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is **linearly independent**. We can say that a unique solution exists for every <img src="svgs/4bdc8d9bcfb35e1c9bfb51fc69687dfc.svg?invert_in_darkmode" align=middle width=7.054855500000005pt height=22.831379999999992pt/> in <img src="svgs/1281caf41453d6d5cb92c8276ef582dd.svg?invert_in_darkmode" align=middle width=23.537085000000005pt height=22.64855999999997pt/>. Let <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/> be the approximate solution, we have <img src="svgs/919579f7983920d5cbb73373ef3ba12c.svg?invert_in_darkmode" align=middle width=143.848485pt height=24.65759999999998pt/>, which means that <img src="svgs/ca301fb24e42abaa21d0d0953010c969.svg?invert_in_darkmode" align=middle width=296.851005pt height=27.91271999999999pt/>. We then obtain the normal equations <img src="svgs/44a881ea5e3bd58126786a8252771223.svg?invert_in_darkmode" align=middle width=103.622145pt height=27.656969999999987pt/> which is a linear system in <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> equations and <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> unknowns. <img src="svgs/4a90a521023b2e61f36dc36d23846cb5.svg?invert_in_darkmode" align=middle width=35.01333pt height=27.656969999999987pt/> is called *Gramian Matrix* of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> and is non-singular, positive definite and symmetric. We can finally get the approximate solution: <img src="svgs/f74ea1e014c464847a638c581437a6f9.svg?invert_in_darkmode" align=middle width=134.05606500000002pt height=27.656969999999987pt/>;

- <img src="svgs/1deb41f00c6b66c372361756ed513507.svg?invert_in_darkmode" align=middle width=139.77744pt height=24.65759999999998pt/>, meaning that there is an **infinite number of solutions**. We can find the least squares solution to the problem by using the **pseudoinverse** obtained via SVD:

  <img src="svgs/528718a8bc34bd1e032413d8d0f63dba.svg?invert_in_darkmode" align=middle width=466.787805pt height=37.921290000000006pt/>

  It is possible to modify the linear least squares problem by introducing a weight matrix with the aim of giving different weights to the components of <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>: <img src="svgs/00780bfbe138c5596faa786b4bb7470f.svg?invert_in_darkmode" align=middle width=188.881605pt height=28.643999999999973pt/> where <img src="svgs/84c95f91a742c9ceb460a83f9b5090bf.svg?invert_in_darkmode" align=middle width=17.808285000000005pt height=22.46574pt/> is an invertible matrix, which gets us to the *weighted normal equations*: <img src="svgs/9517d0df27f057df67e7b3ac2077359b.svg?invert_in_darkmode" align=middle width=169.57000499999998pt height=27.656969999999987pt/>

# Numerical optimization

We've now come at the core of it all: Machine Learning is actually all about **minimization**. In fact, in order to train ML models we need to compute the **best parameters** for the training data set. The thing is: *how do we find what's best?* We can consider *best parameters* as those that minimize (or maximize) the objective function, which is usually called **loss function**. The algorithms that do this are called **optimization algorithms**. Basing on the problem, we can have unconstrained or constrained optimisation: oftentimes the problem imposes some constraints.

Before we talk about **how to find** solutions, we've gotta find out whether **they exist** and are **unique**.

The minimization of a function in several variables can be formulated, with <img src="svgs/12e11ad00c59fe1dc50dfa192568e3ae.svg?invert_in_darkmode" align=middle width=81.77873054999999pt height=22.831056599999986pt/> called **objective function** as minimize <img src="svgs/7997339883ac20f551e7f35efff0a2b9.svg?invert_in_darkmode" align=middle width=31.99783454999999pt height=24.65753399999998pt/> in <img src="svgs/8a86f4a11e2fbfc03de61d587ba826de.svg?invert_in_darkmode" align=middle width=19.998330000000006pt height=22.64855999999997pt/>: this is called an **unconstrained optimization problem**. 

Typically, we want to determine the optimal values of several variables that have constraints, like equality or inequality ones or maybe being lying on a subset of <img src="svgs/8a86f4a11e2fbfc03de61d587ba826de.svg?invert_in_darkmode" align=middle width=19.998330000000006pt height=22.64855999999997pt/>. This kind of optimization is said **constrained** (still pretty obvious) and can be formulated as minimize <img src="svgs/7997339883ac20f551e7f35efff0a2b9.svg?invert_in_darkmode" align=middle width=31.99783454999999pt height=24.65753399999998pt/> in <img src="svgs/846620023f4aedde49b7aba214292d6b.svg?invert_in_darkmode" align=middle width=53.78801174999999pt height=22.648391699999998pt/>. 

Before we actually dive into the topic, let's consider a bit of *context*.

## Preliminaries on multivariate functions

### Partial derivatives and gradients

Let's consider a function <img src="svgs/a199b43c6558f6acfad542bfc46bfcc2.svg?invert_in_darkmode" align=middle width=81.77873054999999pt height=22.831056599999986pt/>. We say that <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is **differentiable with respect to <img src="svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?invert_in_darkmode" align=middle width=14.045955000000003pt height=14.155350000000013pt/>** if the limit <img src="svgs/3e55c35b387ef5ccf963a097ef3c258d.svg?invert_in_darkmode" align=middle width=320.4641863499999pt height=33.20539859999999pt/> exists. A function is said **differentiable at a point <img src="svgs/e714a3139958da04b41e3e607a544455.svg?invert_in_darkmode" align=middle width=15.947580000000002pt height=14.155350000000013pt/>** if and only if **all the partial derivatives** exist in <img src="svgs/e714a3139958da04b41e3e607a544455.svg?invert_in_darkmode" align=middle width=15.947580000000002pt height=14.155350000000013pt/>.

Given a function <img src="svgs/b87d0bc5995a0da558d193116d5ab0a9.svg?invert_in_darkmode" align=middle width=81.77873054999999pt height=22.831056599999986pt/>, the vector containing all of the partial derivatives is called **gradient of <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/>**: <img src="svgs/642d98ce69cba6f89845bde9b098faf9.svg?invert_in_darkmode" align=middle width=222.03491475pt height=37.80850590000001pt/>.

Given a function <img src="svgs/b87d0bc5995a0da558d193116d5ab0a9.svg?invert_in_darkmode" align=middle width=81.77873054999999pt height=22.831056599999986pt/>, **differentiable to the second order**, its **Hessian** matrix <img src="svgs/8e68bda37ca8bf8588a8356ef64e5464.svg?invert_in_darkmode" align=middle width=53.07096629999999pt height=26.76175259999998pt/> is defined as follows:

<img src="svgs/4798a31f46456b193063c352052eef24.svg?invert_in_darkmode" align=middle width=397.04939504999993pt height=120.02612819999997pt/>

Given a function <img src="svgs/a94ee4b0b134dd16bff6331e66faeeb9.svg?invert_in_darkmode" align=middle width=93.44358044999998pt height=22.831056599999986pt/> (note that this time **the output of the function is a vector and not a scalar**), and knowing that all its partial derivatives exist, we call **Jacobian** the following matrix:

<img src="svgs/96cb9b02c43f4bea6e6eec04b566bdc1.svg?invert_in_darkmode" align=middle width=439.71184289999997pt height=106.85030400000001pt/>

If we have two functions <img src="svgs/a94ee4b0b134dd16bff6331e66faeeb9.svg?invert_in_darkmode" align=middle width=93.44358044999998pt height=22.831056599999986pt/> and <img src="svgs/6f723fd2cbd18b563424ef661e886f2d.svg?invert_in_darkmode" align=middle width=90.70697954999999pt height=22.648391699999998pt/> and we **compose** them <img src="svgs/7787f218db1ab235824dd0940dcf7bc6.svg?invert_in_darkmode" align=middle width=111.22673264999999pt height=22.831056599999986pt/>, the Jacobian is given by <img src="svgs/6355234767a26dfb2abc666928ea6431.svg?invert_in_darkmode" align=middle width=188.53215315pt height=24.65753399999998pt/>.

### Minimum points

A point <img src="svgs/3c2727c4cf2d4d29c038f39082bf4884.svg?invert_in_darkmode" align=middle width=57.04141739999999pt height=22.648391699999998pt/> is called a (strict) **local minimum point** if the function is at its lowest value in a range <img src="svgs/7ccca27b5ccc533a2dd72dc6fa28ed84.svg?invert_in_darkmode" align=middle width=6.672451500000003pt height=14.155350000000013pt/>: <img src="svgs/3c263164b285c2f2c33e915f096852e0.svg?invert_in_darkmode" align=middle width=399.4037124pt height=24.65753399999998pt/>.

The same point is called **global minimum** if the *range* is <img src="svgs/8a86f4a11e2fbfc03de61d587ba826de.svg?invert_in_darkmode" align=middle width=19.998330000000006pt height=22.64855999999997pt/>: <img src="svgs/e19058cded1150016e86ca2deeb38b34.svg?invert_in_darkmode" align=middle width=390.2947917pt height=24.65753399999998pt/>.

The **first order optimality condition**, also known as **Fermat's theorem for stationary points**, states that if <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/> is a local **optimum** point and <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is differentiable in <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/>, then the gradient is equal to zero: <img src="svgs/ad617d115f4d7be89728ca413479e1f4.svg?invert_in_darkmode" align=middle width=83.39043569999998pt height=24.65753399999998pt/>.

The **second order optimality condition** states that if <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/> is a local **minimum** point for <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/>, and <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is twice differentiable around <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/>, the <img src="svgs/d143c6d933cdb230fefc1ee70bfbf5f0.svg?invert_in_darkmode" align=middle width=53.25359489999999pt height=24.65753399999998pt/> will be 0 and the <img src="svgs/0d463e61425bc5276794eb3bbfee7cc7.svg?invert_in_darkmode" align=middle width=60.62805539999999pt height=26.76175259999998pt/> is **positive semidefinite**.

Therefore, we can state the final condition for the local minimum points: if <img src="svgs/9e4ac5ba8a6441be421cdcf8fdc78b9d.svg?invert_in_darkmode" align=middle width=81.77873054999999pt height=22.831056599999986pt/> is twice differentiable with **continuity** around <img src="svgs/3c2727c4cf2d4d29c038f39082bf4884.svg?invert_in_darkmode" align=middle width=57.04141739999999pt height=22.648391699999998pt/> and <img src="svgs/c790514b5451c2fefc0770a35805187a.svg?invert_in_darkmode" align=middle width=83.39043569999998pt height=24.65753399999998pt/>, with <img src="svgs/0d463e61425bc5276794eb3bbfee7cc7.svg?invert_in_darkmode" align=middle width=60.62805539999999pt height=26.76175259999998pt/> **positive definite**, then <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/> **is a strict local minimum point**.

### Convexity

Let's consider a set <img src="svgs/990a9650a3a50b518f1c8065344a7525.svg?invert_in_darkmode" align=middle width=53.78801174999999pt height=22.648391699999998pt/>: we can say it is a **convex set** if, for every <img src="svgs/0acac2a2d5d05a8394e21a70a71041b4.svg?invert_in_darkmode" align=middle width=25.350096749999988pt height=14.15524440000002pt/> in the set and a scalar <img src="svgs/1da7ca20b738dda1ab066e8e4ed7db47.svg?invert_in_darkmode" align=middle width=61.141414949999984pt height=24.65753399999998pt/> we have that <img src="svgs/e6059323ea5f293d0eb167f336a0f914.svg?invert_in_darkmode" align=middle width=127.54159109999999pt height=24.65753399999998pt/>.

Now, having a function <img src="svgs/bd2a28bf9cccd082a95a21cb9ba02712.svg?invert_in_darkmode" align=middle width=72.83079209999998pt height=22.831056599999986pt/>, where <img src="svgs/5ea518db22658dd0b99e429bffa3a185.svg?invert_in_darkmode" align=middle width=53.78801174999999pt height=22.648391699999998pt/> is a **convex set**, <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is a **convex function** if, for all the <img src="svgs/58b017e6fd23c18d080e678055639495.svg?invert_in_darkmode" align=middle width=57.313395149999984pt height=22.648391699999998pt/> and <img src="svgs/1da7ca20b738dda1ab066e8e4ed7db47.svg?invert_in_darkmode" align=middle width=61.141414949999984pt height=24.65753399999998pt/>, we have that <img src="svgs/edd6edf0c96f7ffdb4d1122a9d8a3f53.svg?invert_in_darkmode" align=middle width=280.88271749999996pt height=24.65753399999998pt/>.

Finally, we state some properties:

- If <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is **twice differentiable** in <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> and (strictly) **convex**, then <img src="svgs/8e68bda37ca8bf8588a8356ef64e5464.svg?invert_in_darkmode" align=middle width=53.07096629999999pt height=26.76175259999998pt/> is **positive semidefinite** (definite);
- If <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is a **convex** function, then **each point of local minimum is a global minimum**;
- If <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is **strictly convex**, then **a unique point of global minimum exists**.

## Iterative methods

So, what are iterative methods? An iterative method, given an initial vector <img src="svgs/0ed36163119399e052b7ad754f6006f4.svg?invert_in_darkmode" align=middle width=56.85878879999999pt height=22.648391699999998pt/>, computes <img src="svgs/012a46c3097ebfcaad8b89064ffc57f6.svg?invert_in_darkmode" align=middle width=94.74319634999998pt height=24.65753399999998pt/> for <img src="svgs/aa659796b1c899372780b6aa9a77ffc8.svg?invert_in_darkmode" align=middle width=39.21220214999999pt height=22.831056599999986pt/> , **until convergence**. <img src="svgs/ffcbbb391bc04da2d07f7aef493d3e2a.svg?invert_in_darkmode" align=middle width=30.61077854999999pt height=24.65753399999998pt/> is an arbitrary function. What do we mean by convergence? Basically, we have that <img src="svgs/660a2e99c9b64455796148fdfecd6b5f.svg?invert_in_darkmode" align=middle width=59.18371469999999pt height=22.63846199999998pt/> for <img src="svgs/bc72f496744b7fad54a8469890fa37fa.svg?invert_in_darkmode" align=middle width=51.084366299999985pt height=22.831056599999986pt/>, where <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/> is a stationary point, i.e. a local minimum. 

As we're working in the real world, it's pretty obvious that we can't compute an infinite amount of steps . What we can do, though, is computing <img src="svgs/41a0912d0f46af38c7fa2115d8f0386e.svg?invert_in_darkmode" align=middle width=16.661040000000003pt height=14.155350000000013pt/> until we reach a **stopping criterion**, which might be, for example,  an **absolute** one like the norm of the gradient <img src="svgs/43ed9c68013cc404d2628ba58f2360c1.svg?invert_in_darkmode" align=middle width=106.45698854999999pt height=24.65753399999998pt/> or a **relative** one like the same quantity divided by the norm of the function <img src="svgs/669cbbc65c76103ef22999770b87e58b.svg?invert_in_darkmode" align=middle width=98.10462584999999pt height=33.20539859999999pt/>. We might even check the succeding values, in absolute <img src="svgs/f28d4982380568d5ad0bd5c3ae155773.svg?invert_in_darkmode" align=middle width=137.26778009999998pt height=24.65753399999998pt/> and relative <img src="svgs/dc6791b477de0de91b57702197b39199.svg?invert_in_darkmode" align=middle width=111.5083464pt height=33.6803709pt/> ways.

In order to evaluate how fast the algorithm will approximate the solution, we define the **convergence speed** as, considering <img src="svgs/632478dd967d514d6d31a86189f20054.svg?invert_in_darkmode" align=middle width=29.49778919999999pt height=22.465723500000017pt/> as a sequence converging to <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/>:

- **Q-linear** if the distance from the solution reduces by a constant factor <img src="svgs/c85e4a104469a7e07e48d12724b8855a.svg?invert_in_darkmode" align=middle width=64.49382884999999pt height=24.65753399999998pt/> at <img src="svgs/bc72f496744b7fad54a8469890fa37fa.svg?invert_in_darkmode" align=middle width=51.084366299999985pt height=22.831056599999986pt/>: <img src="svgs/7b19f77f4283efba3b7468738f00b4d0.svg?invert_in_darkmode" align=middle width=222.43985444999996pt height=33.989986799999976pt/>
- **Q-superlinear** if the same limit converges to <img src="svgs/29632a9bf827ce0200454dd32fc3be82.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>: <img src="svgs/a78c71aa30081a4dadde27045d347ee9.svg?invert_in_darkmode" align=middle width=160.45747244999998pt height=33.989986799999976pt/>
- **Q-quadratic** if it is even faster, in fact <img src="svgs/0a303f351e87e85870da0fe05d11ac9b.svg?invert_in_darkmode" align=middle width=170.7112836pt height=33.989986799999976pt/>

The Q stands for **Q**uotient because, well, they're quotients 

Remember that these methods converge to a **stationary point**, but that **doesn't always mean** it is a global minimum too.

## Descent methods

The most useful subset of iterative methods is the one containing **descent methods**. These, given an initial vector <img src="svgs/0ed36163119399e052b7ad754f6006f4.svg?invert_in_darkmode" align=middle width=56.85878879999999pt height=22.648391699999998pt/>, compute <img src="svgs/93f1b5750a975eb1a95fcd143ac06aa2.svg?invert_in_darkmode" align=middle width=127.75878389999998pt height=19.1781018pt/> for <img src="svgs/aa659796b1c899372780b6aa9a77ffc8.svg?invert_in_darkmode" align=middle width=39.21220214999999pt height=22.831056599999986pt/> until convergence. Here, <img src="svgs/3cc1484752cc8ef69d55b6991e28be35.svg?invert_in_darkmode" align=middle width=17.78167709999999pt height=14.15524440000002pt/> is called **stepsize** and <img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/> is the **descent direction**. It is effectively called *descent direction* if, for a given <img src="svgs/7bf7e8d5e8bb2d529064abede4641de4.svg?invert_in_darkmode" align=middle width=46.913938499999986pt height=21.18721440000001pt/>, we have <img src="svgs/ee2a06e6f639ed04df1785305f85288f.svg?invert_in_darkmode" align=middle width=157.14248055pt height=24.65753399999998pt/>

Provided that <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is continuously differentiable around <img src="svgs/41a0912d0f46af38c7fa2115d8f0386e.svg?invert_in_darkmode" align=middle width=16.661040000000003pt height=14.155350000000013pt/>, we have that <img src="svgs/74344c4e17e795a40b5784543ec48152.svg?invert_in_darkmode" align=middle width=102.54745049999998pt height=27.6567522pt/> when <img src="svgs/15b7dd7aa70cb590ab33f432268acd3d.svg?invert_in_darkmode" align=middle width=83.92129019999999pt height=24.65753399999998pt/> and <img src="svgs/9db0138df7a05845b3222c4e2424e831.svg?invert_in_darkmode" align=middle width=46.49535164999998pt height=21.18721440000001pt/> when <img src="svgs/6858784be4eded4670129fe9bcea72f2.svg?invert_in_darkmode" align=middle width=83.92129019999999pt height=24.65753399999998pt/>. This basically means that the direction *goes towards* the stationary points.

Note that **the choice of the stepsize is crucial**: a small stepsize could make the descent too slow, while a large stepsize can make the convergence impossible. 

The stepsize is therefore chosen by a *line search* algorithm, implemented with a backtracking algorithm, where an iterative procedure decreases the value of the step size <img src="svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width=10.576500000000003pt height=14.155350000000013pt/> until suitable conditions for the convergence of the method are satisfied. Why *line search*? Because the new iterate <img src="svgs/eaf657bf932d6276b08b6d1ac23782bc.svg?invert_in_darkmode" align=middle width=33.304938149999984pt height=14.15524440000002pt/> is searched along the line <img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/>.

We can distinguish between **exact line search**, where <img src="svgs/3cc1484752cc8ef69d55b6991e28be35.svg?invert_in_darkmode" align=middle width=17.78167709999999pt height=14.15524440000002pt/> is chosen as the minimum of the function <img src="svgs/49428feea9cf3a9b3c66a2818db79e8d.svg?invert_in_darkmode" align=middle width=208.92647159999999pt height=24.65753399999998pt/> (which is pretty slow) and **inexact line search**, where <img src="svgs/3cc1484752cc8ef69d55b6991e28be35.svg?invert_in_darkmode" align=middle width=17.78167709999999pt height=14.15524440000002pt/> is chosen to belong to intervals where the convergence is guaranteed. The most common conditions that guarantee this are the **Wolfe** conditions. 

The first one, named **Armijo condition** is the following: <img src="svgs/73bb969cb7a76fb07d88309a0d7badd0.svg?invert_in_darkmode" align=middle width=281.97507195pt height=27.6567522pt/>. This ensures that <img src="svgs/3cc1484752cc8ef69d55b6991e28be35.svg?invert_in_darkmode" align=middle width=17.78167709999999pt height=14.15524440000002pt/> is not too large, but it allows it to be really small. Another way of describing this condition is to say that the decrease in the objective function should be proportional to both the step length and the directional derivative of the function and step direction. In general, <img src="svgs/988584bba6844388f07ea45b7132f61c.svg?invert_in_darkmode" align=middle width=13.666351049999989pt height=14.15524440000002pt/> is a very small value, <img src="svgs/3d3683fb6d2b11229b0bad3193afebcb.svg?invert_in_darkmode" align=middle width=33.2649867pt height=26.76175259999998pt/>.

Since this first condition allows <img src="svgs/888b6c2a06fc366952ac84a80c43f5f7.svg?invert_in_darkmode" align=middle width=15.95518319999999pt height=14.15524440000002pt/> to be very small, we should limit this. The second condition, also known as the **curvature condition**, does exactly so: <img src="svgs/23230ac297a962f615fb1f42b77eb000.svg?invert_in_darkmode" align=middle width=251.63438024999994pt height=27.6567522pt/>. Since this condition is computationally expensive, we can just apply a backtracking algorithm that progressively reduces a starting value of <img src="svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width=10.576500000000003pt height=14.155350000000013pt/> until the Armijo condition is satisfied. The algorithm should check if <img src="svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width=10.576500000000003pt height=14.155350000000013pt/> becomes to small: in that case, it just stops.

### Gradient descent method

The **gradient descent method** is basically what we're here for. It is a first order optimization where the descent direction is set as <img src="svgs/4d59ca387b4bc6d8b7895179e5fada43.svg?invert_in_darkmode" align=middle width=104.84602589999997pt height=24.65753399999998pt/>. This means that we get <img src="svgs/eaf657bf932d6276b08b6d1ac23782bc.svg?invert_in_darkmode" align=middle width=33.304938149999984pt height=14.15524440000002pt/> as <img src="svgs/b8c5ca261305febab9182679860565a1.svg?invert_in_darkmode" align=middle width=101.93507114999998pt height=24.65753399999998pt/>.

The convergence speed is **linear**. To speed things up a bit, we introduce what we call **momentum**. This means that a *memory* part is added to the equation, so that the formula to update <img src="svgs/41a0912d0f46af38c7fa2115d8f0386e.svg?invert_in_darkmode" align=middle width=16.661040000000003pt height=14.155350000000013pt/> becomes: <img src="svgs/359ae64c6478eeb8d2f814b055b8b38c.svg?invert_in_darkmode" align=middle width=253.11004620000003pt height=27.6567522pt/>, where <img src="svgs/c5ac6c3d003721d35a16d95f12d67745.svg?invert_in_darkmode" align=middle width=27.74455859999999pt height=22.465723500000017pt/> is just the distance between the last <img src="svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?invert_in_darkmode" align=middle width=14.045955000000003pt height=14.155350000000013pt/> and the preceding one: <img src="svgs/10c45db0e0fbb00324c03c9a226f31eb.svg?invert_in_darkmode" align=middle width=298.54202234999997pt height=27.6567522pt/>.

### Newton method

The **Newton method** uses second order information of <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/>, the **Hessian**.

The direction is set as <img src="svgs/3fb9548de396922803fce921ccd9a6d3.svg?invert_in_darkmode" align=middle width=167.76281774999998pt height=28.894955100000008pt/>, provided that <img src="svgs/1a7cc37c97773d9666f01cb0557bb974.svg?invert_in_darkmode" align=middle width=21.36426929999999pt height=22.465723500000017pt/> is positive definite, within a sufficiently large range of <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/>. At each iteration we compute <img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/> as a solution of the linear system <img src="svgs/740805b11d59544b6f1f9faa63668b72.svg?invert_in_darkmode" align=middle width=144.5151378pt height=24.65753399999998pt/>. 

We know that <img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/> is a descent direction only if the Hessian <img src="svgs/1a7cc37c97773d9666f01cb0557bb974.svg?invert_in_darkmode" align=middle width=21.36426929999999pt height=22.465723500000017pt/> is positive definite, and the **convergence properties** are strictly related to it. In fact, if <img src="svgs/1776ca1e64d9b4f191a0a0da95714076.svg?invert_in_darkmode" align=middle width=74.86526024999999pt height=26.76175259999998pt/>, <img src="svgs/1a7cc37c97773d9666f01cb0557bb974.svg?invert_in_darkmode" align=middle width=21.36426929999999pt height=22.465723500000017pt/> is a Liptschiz function around <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/> (which means that <img src="svgs/9c63d91a01575d785c981599195e3d50.svg?invert_in_darkmode" align=middle width=184.10196089999997pt height=24.65753399999998pt/>) and <img src="svgs/1a7cc37c97773d9666f01cb0557bb974.svg?invert_in_darkmode" align=middle width=21.36426929999999pt height=22.465723500000017pt/> is positive definite we can say that if <img src="svgs/e714a3139958da04b41e3e607a544455.svg?invert_in_darkmode" align=middle width=15.947580000000002pt height=14.155350000000013pt/> is near a stationary point <img src="svgs/1da57587812d6070f08b912a6488a939.svg?invert_in_darkmode" align=middle width=16.130235000000003pt height=22.638659999999973pt/>, the sequence **converges quadratically**. Which is fast.

To speed things up furthermore, we could approximate <img src="svgs/1a7cc37c97773d9666f01cb0557bb974.svg?invert_in_darkmode" align=middle width=21.36426929999999pt height=22.465723500000017pt/> with a positive definite matrix <img src="svgs/c6f06b8f80dbe4f5e6e1bceb54e0cf11.svg?invert_in_darkmode" align=middle width=51.25879604999998pt height=24.65753399999998pt/>, and solve the linear system to compute the direction <img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/> with **iterative methods** instead of the direct ones.

### Just one more thing

Remember that the starting guess <img src="svgs/e714a3139958da04b41e3e607a544455.svg?invert_in_darkmode" align=middle width=15.947580000000002pt height=14.155350000000013pt/> has a crucial role: the algorithm converges to a local minimum, so <img src="svgs/e714a3139958da04b41e3e607a544455.svg?invert_in_darkmode" align=middle width=15.947580000000002pt height=14.155350000000013pt/> determines the local minimum it will descend to. Obviously, we don't know where the global is, but if we have an estimation of where it might be we can choose <img src="svgs/e714a3139958da04b41e3e607a544455.svg?invert_in_darkmode" align=middle width=15.947580000000002pt height=14.155350000000013pt/> as near as possible. In any case, there's no guarantee that we'll fall into the right minimum 

## Convex optimization

Convex quadratic functions take the following form: <img src="svgs/57a09113cd3b987c9e2bb1a1a0447c81.svg?invert_in_darkmode" align=middle width=189.40215855pt height=27.77565449999998pt/>, where <img src="svgs/17d7f7901900ad89f39ac6b3cbe174e5.svg?invert_in_darkmode" align=middle width=63.904014899999986pt height=22.648391699999998pt/> and <img src="svgs/e97fff59a5fe98b8ebf80052e123577e.svg?invert_in_darkmode" align=middle width=71.48480955pt height=26.17730939999998pt/> is symmetric positive definite. A typical quadratic optimization problem is the **least square problem**: *min*<img src="svgs/4da871a88c7da113cdac2f2f266d6436.svg?invert_in_darkmode" align=middle width=71.86074059999999pt height=26.76175259999998pt/>. The objective is to **minimize** <img src="svgs/bf88b266aef9e46573e833b51cd8e4a8.svg?invert_in_darkmode" align=middle width=458.14277519999996pt height=27.77565449999998pt/>, which can be **rewritten in quadratic form** by setting <img src="svgs/85a0b255f8516d0caafb76a023c9b5ed.svg?invert_in_darkmode" align=middle width=69.92624429999998pt height=27.6567522pt/> and <img src="svgs/24b5c5fe91fed2679713fa242c1632ad.svg?invert_in_darkmode" align=middle width=82.57827434999999pt height=27.6567522pt/>. 

We know that the **gradient** <img src="svgs/5f04217792d71868b72a3e7de72784dd.svg?invert_in_darkmode" align=middle width=45.69650579999999pt height=24.65753399999998pt/> is <img src="svgs/c4d7806fcf6e1c9713ed8e1821942096.svg?invert_in_darkmode" align=middle width=95.57180819999998pt height=27.6567522pt/>, therefore when we set <img src="svgs/8e7f3a2c7f835028447a8e99b7517f0d.svg?invert_in_darkmode" align=middle width=75.83334659999998pt height=24.65753399999998pt/> we obtain the normal equation <img src="svgs/2fc08a85c5dac7a78790ce76091be2bb.svg?invert_in_darkmode" align=middle width=110.85030824999998pt height=27.6567522pt/>.

A strictly convex function that is frequently encountered in applications is the **quadratic function** <img src="svgs/62aa81446f6a309756bd608564c0278a.svg?invert_in_darkmode" align=middle width=153.45076394999998pt height=27.77565449999998pt/>, where <img src="svgs/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode" align=middle width=12.99542474999999pt height=22.465723500000017pt/> is a **square matrix** of size <img src="svgs/2be744f3276b5219af5f8dd5f793e02c.svg?invert_in_darkmode" align=middle width=39.82506pt height=19.178279999999994pt/>, symmetric and positive definite, and <img src="svgs/28bd07fe926e4a110e2804fa8f2a4b60.svg?invert_in_darkmode" align=middle width=47.14413659999998pt height=22.831056599999986pt/>. In this particular case we have that the gradient <img src="svgs/5f04217792d71868b72a3e7de72784dd.svg?invert_in_darkmode" align=middle width=45.69650579999999pt height=24.65753399999998pt/> is <img src="svgs/b4be0667a04fcb0793f4e75efc0f2c28.svg?invert_in_darkmode" align=middle width=49.53640064999999pt height=22.831056599999986pt/>, and the second order gradient <img src="svgs/8e68bda37ca8bf8588a8356ef64e5464.svg?invert_in_darkmode" align=middle width=53.07096629999999pt height=26.76175259999998pt/> is simply <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>.

This leads us to the fact that the unique minimizer of <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> is the solution of the linear system representing <img src="svgs/8e7f3a2c7f835028447a8e99b7517f0d.svg?invert_in_darkmode" align=middle width=75.83334659999998pt height=24.65753399999998pt/>, which is <img src="svgs/0db18eae2019e3dca2330ba26c392004.svg?invert_in_darkmode" align=middle width=51.362840099999985pt height=22.831056599999986pt/>.

### Steepest descent method

The **steepest descent method** is the gradient method applied to a quadratic convex function with **exact line search**. The step length <img src="svgs/3cc1484752cc8ef69d55b6991e28be35.svg?invert_in_darkmode" align=middle width=17.78167709999999pt height=14.15524440000002pt/> is computed as the global minimum of the variable <img src="svgs/c745b9b57c145ec5577b82542b2df546.svg?invert_in_darkmode" align=middle width=10.576500000000003pt height=14.155350000000013pt/> (in this case the function is convex) <img src="svgs/cfbe8e143ba8f3bd1ec07ecb228ba69b.svg?invert_in_darkmode" align=middle width=123.62694959999997pt height=28.92634470000001pt/>, which leads us to <img src="svgs/ff461dfcb3cc899498c06b0adf3450e0.svg?invert_in_darkmode" align=middle width=191.5117842pt height=37.099524pt/>.

Finally, the algorithm will be something like:

1. <img src="svgs/ed7ffedd3b5e98cb888b00b35fafb50b.svg?invert_in_darkmode" align=middle width=93.44997419999999pt height=22.831056599999986pt/>
2. <img src="svgs/0d57ac54a912f0da6ae4b6ba76862b09.svg?invert_in_darkmode" align=middle width=82.36122179999998pt height=37.92139230000001pt/> 
3. <img src="svgs/955e6479200859d184d5afc556ba84f1.svg?invert_in_darkmode" align=middle width=203.83535369999998pt height=22.831056599999986pt/>

The gradient can be modified by considering Q-conjugate directions <img src="svgs/0d19b0a4827a28ecffa01dfedf5f5f2c.svg?invert_in_darkmode" align=middle width=12.92146679999999pt height=14.15524440000002pt/>: <img src="svgs/fcd23133b64555fbe4117f2857db0a80.svg?invert_in_darkmode" align=middle width=150.19186544999997pt height=27.6567522pt/>.

We even cite the **Conjugate Gradient** algorithm, a very fast iterative algorithm for the minimization of a quadratic function (i.e. it solves a linear system <img src="svgs/0db18eae2019e3dca2330ba26c392004.svg?invert_in_darkmode" align=middle width=51.362840099999985pt height=22.831056599999986pt/> with <img src="svgs/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode" align=middle width=12.99542474999999pt height=22.465723500000017pt/> symmetric and positive definite).

<img src="svgs/916bff1ff88e4522d91ba0f92423ba6a.svg?invert_in_darkmode" align=middle width=261.6685632pt height=147.28732920000002pt/>

If A is symmetric and positive definite, with at most <img src="svgs/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode" align=middle width=7.7054801999999905pt height=14.15524440000002pt/> distinct eigenvalues, then the CG method **converges** in at most **<img src="svgs/6f9bad7347b91ceebebd3ad7e6f6f2d1.svg?invert_in_darkmode" align=middle width=7.7054801999999905pt height=14.15524440000002pt/> iterations**!

For any symmetric and positive definite matrix <img src="svgs/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode" align=middle width=12.99542474999999pt height=22.465723500000017pt/>, we define the *energy norm* <img src="svgs/eaef5fa0c9451966a5d389926c15b731.svg?invert_in_darkmode" align=middle width=117.50483745pt height=30.62114220000002pt/>. Concerning the convergence speed of gradient descent and conjugate gradient algorithms when they're applied to a quadratic function, we can say that the CG method is quite faster than the gradient descent: the steepest gradient descent holds <img src="svgs/4b1c471e31085b1d11dd6cec401569af.svg?invert_in_darkmode" align=middle width=287.52760905pt height=44.51174640000002pt/>, while the CG method holds <img src="svgs/02d02637b9b47362490172046ed3e8cc.svg?invert_in_darkmode" align=middle width=280.15315019999997pt height=37.80850590000001pt/>, where <img src="svgs/63bb9849783d01d91403bc9a5fea12a2.svg?invert_in_darkmode" align=middle width=9.075495000000004pt height=22.831379999999992pt/> is the iteration number and <img src="svgs/a41df9f6b83763fcad030f544be4b1a6.svg?invert_in_darkmode" align=middle width=40.91785829999999pt height=24.65753399999998pt/> is the condition number of <img src="svgs/1afcdb0f704394b16fe85fb40c45ca7a.svg?invert_in_darkmode" align=middle width=12.99542474999999pt height=22.465723500000017pt/>.

## Stochastic optimization

The objective function in ML problems is sometimes related to **probabilistic events**, and it can take the following form <img src="svgs/30175f2f7339b3080e87956a2b2675f0.svg?invert_in_darkmode" align=middle width=139.8085755pt height=26.438629799999987pt/>, where <img src="svgs/b3ca9d3a87ebc68a82ee922451036074.svg?invert_in_darkmode" align=middle width=84.88596104999999pt height=22.648391699999998pt/> is the loss function computed at the <img src="svgs/a72a930a13c04c176e62260f08c7ae81.svg?invert_in_darkmode" align=middle width=26.667854399999992pt height=22.831056599999986pt/> observation of the dataset.

For example, in a **supervised classification problem** we'll want to minimize the **empirical risk function**, i.e. the sum of loss functions for all the examples <img src="svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?invert_in_darkmode" align=middle width=14.045955000000003pt height=14.155350000000013pt/>: <img src="svgs/3047d40026f6026e96a1978a99d6ea93.svg?invert_in_darkmode" align=middle width=200.01705569999996pt height=27.77565449999998pt/>, where the loss function <img src="svgs/2f2322dff5bde89c37bcae4116fe20a8.svg?invert_in_darkmode" align=middle width=5.2283516999999895pt height=22.831056599999986pt/> is related to the probability distribution of the examples <img src="svgs/9fc20fb1d3825674c6a279cb0d5ca636.svg?invert_in_darkmode" align=middle width=14.045955000000003pt height=14.155350000000013pt/>.

In the case of a **standard gradient descent**, the iteration is <img src="svgs/4b59c70c97248d1936b3cbcb494584a5.svg?invert_in_darkmode" align=middle width=170.10128684999998pt height=24.65753399999998pt/>, which means that we have to calculate the gradient for every sample: <img src="svgs/77edd657a87b4a051dfb417d580f4d81.svg?invert_in_darkmode" align=middle width=163.48986885pt height=26.438629799999987pt/>.

As everyone can imagine, if we have millions of samples in the batch this is crazily difficult to compute

In **stochastic gradient descent**, the true gradient is approximated at each iteration by the gradient at a **single observation**, randomly picked. The iteration step becomes <img src="svgs/5fbc7a60f0290d9b73e88e1312782682.svg?invert_in_darkmode" align=middle width=174.58666499999998pt height=24.65753399999998pt/>.

A compromise between the SGD and the *standard* gradient descent is using a **mini-batch** of randomly picked samples. The step size <img src="svgs/3cc1484752cc8ef69d55b6991e28be35.svg?invert_in_darkmode" align=middle width=17.78167709999999pt height=14.15524440000002pt/> can be fixed or even computed with a line search procedure. Each iteration is now **very cheap**, and <img src="svgs/d272efe2a3329ae8f1b3202e6d72a693.svg?invert_in_darkmode" align=middle width=69.95988119999998pt height=14.15524440000002pt/> is a stochastic process, with a behaviour defined by the random sequence of <img src="svgs/77a3b857d53fb44e33b53e4c8b68351a.svg?invert_in_darkmode" align=middle width=5.663295000000005pt height=21.683310000000006pt/>s. Even if the direction might not be a real descent direction, it is a descent direction in **expectation**, which is proved to converge *in expectation* to a local minimum. The convergence of SGD has been analyzed using the theories behind convex minimization and stochastic approximation; since ML doesn't require a super precise localization of the minimum, SGD is a tremendously good compromise between accuracy and speed.

## Non-linear least squares problem

In non-linear least squares problem we get residuals defined as a **non-linear function** <img src="svgs/b910edbb0207a667a578721b3a48a923.svg?invert_in_darkmode" align=middle width=91.49912309999999pt height=22.648391699999998pt/>, where <img src="svgs/55a049b8f161ae7cfeb0197d75aff967.svg?invert_in_darkmode" align=middle width=9.867000000000003pt height=14.155350000000013pt/> is the dimension of the space of the data and <img src="svgs/0e51a2dede42189d77627c4d742822c3.svg?invert_in_darkmode" align=middle width=14.433210000000003pt height=14.155350000000013pt/> is the dimension of the space of the unknowns. The minimization of these residuals can be written as <img src="svgs/30ad01d0089f6c26d26d19c80cbf0cfe.svg?invert_in_darkmode" align=middle width=212.95408695pt height=26.76175259999998pt/>. We can therefore write the **gradient** of the function <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> representing the residuals <img src="svgs/07a044c48db259c954b1cd91908c7530.svg?invert_in_darkmode" align=middle width=217.23173505pt height=27.77565449999998pt/>, which is <img src="svgs/ea75c10b95a2cef233f958e2c69c56ea.svg?invert_in_darkmode" align=middle width=300.73525679999995pt height=27.6567522pt/>. 

We can even compute the **Hessian** of <img src="svgs/190083ef7a1625fbc75f243cffb9c96d.svg?invert_in_darkmode" align=middle width=9.817500000000004pt height=22.831379999999992pt/> as <img src="svgs/4dde3ed7641817df48b965df012c3a0d.svg?invert_in_darkmode" align=middle width=373.90072829999997pt height=107.8769703pt/>

As you can imagine, these formula can be pretty harsh to compute. The methods we can use to solve these problems are the *gradient methods, Newton-like methods, the Gauss-Newton method* (where <img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/> is computed as <img src="svgs/37564183c1427ce7fa94637212b56104.svg?invert_in_darkmode" align=middle width=265.86660045pt height=27.94539330000001pt/>), or *the Levenberh-Marquardt method*, which adds regularization because the product of the two Jacobians can be ill-conditioned (<img src="svgs/a28020cb9b58a3a875adec3adf5d824a.svg?invert_in_darkmode" align=middle width=15.536730000000006pt height=14.155350000000013pt/> is computed as <img src="svgs/fe9365ee7eacd9927b94b6013e4de25d.svg?invert_in_darkmode" align=middle width=335.0730932999999pt height=27.94539330000001pt/>).

## Constrained optimization, Lagrange multipliers

We finally consider the problem <img src="svgs/82d573a0287b62716cc67ecf8ee088e1.svg?invert_in_darkmode" align=middle width=70.77867554999999pt height=24.65753399999998pt/>, with <img src="svgs/0f6b82136f85e061b25a0635e0ff8651.svg?invert_in_darkmode" align=middle width=159.50319825pt height=24.65753399999998pt/>.

A possible way of solving this is the introduction of *Lagrange multipliers* <img src="svgs/704b9cf622dc5eb0d3d5c1056d1b74d4.svg?invert_in_darkmode" align=middle width=45.198718949999986pt height=22.831056599999986pt/>. The *Lagrangian function* associated to the problem is <img src="svgs/e8fbb5155a7c0d55514ae309527c8344.svg?invert_in_darkmode" align=middle width=221.77074645000002pt height=26.438629799999987pt/>.

Many numerical methods also use the concept of duality. The minimization in a set of variables, let's say <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>, is transformed into the minimization of another set of variables, like <img src="svgs/fd8be73b54f5436a5cd2e73ba9b6bfa9.svg?invert_in_darkmode" align=middle width=9.589140000000002pt height=22.831379999999992pt/>. 

We can associate the **Lagrangian dual problem** to the *primal problem*, in the form of <img src="svgs/436d52f123382dc41500febd516678e2.svg?invert_in_darkmode" align=middle width=105.38876039999998pt height=24.65753399999998pt/> such that <img src="svgs/6bd0fbad9a94269e351ee749d8d8cce1.svg?invert_in_darkmode" align=middle width=39.72592304999999pt height=22.831056599999986pt/>, where <img src="svgs/528af6a9376625e0ac78cf28b5c80a96.svg?invert_in_darkmode" align=middle width=171.17623214999998pt height=24.65753399999998pt/> and <img src="svgs/fd8be73b54f5436a5cd2e73ba9b6bfa9.svg?invert_in_darkmode" align=middle width=9.589140000000002pt height=22.831379999999992pt/> are the **dual variables**.

The first one (<img src="svgs/78ec2b7008296ce0561cf83393cb746d.svg?invert_in_darkmode" align=middle width=14.066250000000002pt height=22.46574pt/>) is the **minimax problem**; if the solution can be easily performed, then the problem is easy to solve. In fact, the maximization problem of <img src="svgs/d2112b27124f2e2a2658ab62cdcebe86.svg?invert_in_darkmode" align=middle width=36.44074829999999pt height=24.65753399999998pt/> is a **concave problem**, which has an easy to find maximum, even if <img src="svgs/5201385589993766eea584cd3aa6fa13.svg?invert_in_darkmode" align=middle width=12.92464304999999pt height=22.465723500000017pt/> and <img src="svgs/ddd3bc35b936d6a00e6a81cab0061f32.svg?invert_in_darkmode" align=middle width=14.12201339999999pt height=22.831056599999986pt/> are not convex.

A particular case is when the functions <img src="svgs/5201385589993766eea584cd3aa6fa13.svg?invert_in_darkmode" align=middle width=12.92464304999999pt height=22.465723500000017pt/> and <img src="svgs/ddd3bc35b936d6a00e6a81cab0061f32.svg?invert_in_darkmode" align=middle width=14.12201339999999pt height=22.831056599999986pt/> are linear: this is called the **linear programming problem**.

# Probability and statistics

The objective of probability is to **quantify the uncertainty**. We want to first introduce the concepts of random variables and probability distributions. Cool shit. When working in ML, probability is often used to formalize the design of automated reasoning systems. We are also interested in the errors an algorithm produces. We can distinguish between two interpretations of probability: the **Bayesian** interpretation and the **frequentist** interpretation. The first one is also called *subjective probability* because it is used to quantify the degree of uncertainty the user has about an event. The second one considers the frequency of events related to the total number of them. 

## Basics

Here we are, back to basics. Let's define a **random experiment** as an experiment which has an outcome *determined by chance*. The **sample space** is simply the set of all possible outcomes of the experiment. An **event** is a collection of results, i.e. a **subset** of the sample space. The **space of the events** <img src="svgs/230abac77a7c5fe7d20eb971e9b69a2b.svg?invert_in_darkmode" align=middle width=36.14161484999999pt height=24.65753399999998pt/> is the set of all possible events. The **probability of an event <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>** is a function <img src="svgs/7a99efe49b4b1ed4b11af41a6cccf74b.svg?invert_in_darkmode" align=middle width=153.0874752pt height=24.65753399999998pt/> that associates each event <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> to a number, called **probability** of <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/>.

Each **probability function** satisfies 3 properties:

- <img src="svgs/e7c30c75536a7fe4b759efdf702d784b.svg?invert_in_darkmode" align=middle width=68.08784399999999pt height=24.65753399999998pt/>, which means that a probability can't be negative;
- <img src="svgs/57ef9610de47e9da1f80a01714991712.svg?invert_in_darkmode" align=middle width=66.78642794999999pt height=24.65753399999998pt/> , which means that the probability of one of **all the possible outcomes** is 100%
- Given <img src="svgs/21960040b28b98e46bfef0f7d0db735e.svg?invert_in_darkmode" align=middle width=76.6874955pt height=22.465723500000017pt/> disjoint events, the probability of any of them happening is the sum of all the probabilities: <img src="svgs/68f8043bc40ca541683361ddbad6f0b9.svg?invert_in_darkmode" align=middle width=273.3062739pt height=26.438629799999987pt/>

The **conditional probability** of an event <img src="svgs/61e84f854bc6258d4108d08d4c4a0852.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=22.46574pt/> given the event <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is <img src="svgs/8eab1668c734d1bb2ff6fbe424b6b6ae.svg?invert_in_darkmode" align=middle width=129.35063459999998pt height=33.20539859999999pt/>.

For any fixed <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> having <img src="svgs/789d78767dee7880eeeb40fbbd4af18c.svg?invert_in_darkmode" align=middle width=68.08784399999999pt height=24.65753399999998pt/>, we know that <img src="svgs/ae3009abcef7580ddea6b068845490d4.svg?invert_in_darkmode" align=middle width=85.94747204999999pt height=24.65753399999998pt/> for every <img src="svgs/61e84f854bc6258d4108d08d4c4a0852.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=22.46574pt/> in the sample space. We know that <img src="svgs/8d3d9438b76974eaa546b7caebc154a7.svg?invert_in_darkmode" align=middle width=83.68145114999999pt height=24.65753399999998pt/>, and given <img src="svgs/3856e6b46b3345a0015017fda5cbf22f.svg?invert_in_darkmode" align=middle width=76.96720184999998pt height=22.465723500000017pt/> disjoint events, we know that the probability of one of them happening given <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> is the sum of all the probabilities given A: <img src="svgs/1abe372cd629160dd860c5bdfaf71f4d.svg?invert_in_darkmode" align=middle width=247.34171384999996pt height=26.438629799999987pt/>. We even know that <img src="svgs/5ad670081bd67563b24c77e1a9f37d6d.svg?invert_in_darkmode" align=middle width=85.81994354999999pt height=22.831056599999986pt/> events, <img src="svgs/b624add46b763f057d0615eb7d8244e7.svg?invert_in_darkmode" align=middle width=498.02951385pt height=24.65753399999998pt/>, which means that the probability of them happening all at the same time is the multiplication of all the conditional probabilities.

Two events <img src="svgs/53d147e7f3fe6e47ee05b88b166bd3f6.svg?invert_in_darkmode" align=middle width=12.328800000000005pt height=22.46574pt/> and <img src="svgs/61e84f854bc6258d4108d08d4c4a0852.svg?invert_in_darkmode" align=middle width=13.293555000000003pt height=22.46574pt/> are **independent** if and only if the probability of them happening at the same time is the multiplication of the single probabilities: <img src="svgs/ce0ee62b34105c59f32b993ef245252d.svg?invert_in_darkmode" align=middle width=180.16533149999998pt height=24.65753399999998pt/>.

Given two disjoint events <img src="svgs/91daf49251530f97b200e0d037770c11.svg?invert_in_darkmode" align=middle width=32.92809134999999pt height=22.465723500000017pt/>, with the union of them being the sample space, we have that <img src="svgs/dc7383f41b7ef5a4af6a7420e868a3c1.svg?invert_in_darkmode" align=middle width=155.30165144999998pt height=33.20539859999999pt/>. This theorem, known as the **Bayes' theorem**, can be extended to multiple events, pairwise disjoint and exhaustive: <img src="svgs/16d67c12c460e640b7efb6fa0f91dc9e.svg?invert_in_darkmode" align=middle width=218.01548504999994pt height=33.20539859999999pt/>.

## Random variables

A **random variable** associates events to numbers. This helps us in the mathematical description of the probabilities. Mathematically speaking, a **random variable** is a function <img src="svgs/670d3ee028ff7f44c8356a52c2d6ba91.svg?invert_in_darkmode" align=middle width=77.07725024999998pt height=22.648391699999998pt/> that associates each outcome <img src="svgs/dd3347e602934e981a3ccd717b9e1ed2.svg?invert_in_darkmode" align=middle width=43.32938609999999pt height=22.465723500000017pt/> to a number <img src="svgs/4181e2d7852d676869d94eba697f73a2.svg?invert_in_darkmode" align=middle width=41.35830434999999pt height=22.648391699999998pt/>: <img src="svgs/08b64bf0a4b2e2c45dbc994ba70238bf.svg?invert_in_darkmode" align=middle width=71.21756399999998pt height=24.65753399999998pt/>. The set of all the possible values of the random variable is called **target space** (aka **support**) of <img src="svgs/e051edd819a2fa6330999e7920028d0b.svg?invert_in_darkmode" align=middle width=43.05595634999999pt height=22.465723500000017pt/>.

If the target space is a countable set, the random variable is said **discrete**. If it is not countable, we're working with a **continuous RV**.

We can associate to the RV probability functions, that describe how the probabilities are distributed. We'll talk about **univariate** distributions when there's a single RV involved, and **multivariate** when there's more.

### Probability Mass Function

Each **discrete** RV has a function associated to itself, called **Probability Mass Function** (PMF), which describes the mapping between the event and the outcome: <img src="svgs/53994fa0e45a781ed75e1787e3be87b6.svg?invert_in_darkmode" align=middle width=147.45282419999998pt height=24.65753399999998pt/>, such that the function takes a value as input and outputs the probability that the RV has that value: <img src="svgs/7ed1dc59012f25240e66ab07b50e83d0.svg?invert_in_darkmode" align=middle width=195.25539164999998pt height=24.65753399999998pt/>. Obviously, the sum of all the possible outputs will be equal to <img src="svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/>.

The **expectation** of a discrete RV is defined as <img src="svgs/78c1bdb8df425e3387c75d65a340cf99.svg?invert_in_darkmode" align=middle width=196.7577183pt height=24.657735299999988pt/>.

We can extend the concept **to functions**, so that <img src="svgs/c7263d98a992275fd015fdb8fd8fc8a6.svg?invert_in_darkmode" align=middle width=207.58932644999996pt height=24.657735299999988pt/>.

The **variance** of a discrete RV is defined as <img src="svgs/055c9d564c15a16e3dd382069946c416.svg?invert_in_darkmode" align=middle width=124.43103584999997pt height=26.76175259999998pt/>, or simply <img src="svgs/8f3a97bf422d3932d0e4f51e67d214dc.svg?invert_in_darkmode" align=middle width=104.84022119999997pt height=26.76175259999998pt/>.

Some examples of PMF might be the *discrete uniform distribution* <img src="svgs/179b80cc86f52ed7205e115c2a3ddc1b.svg?invert_in_darkmode" align=middle width=8.126022299999999pt height=27.77565449999998pt/>, or the Poisson <img src="svgs/a59a22a9253985cdc0df0fa37f308765.svg?invert_in_darkmode" align=middle width=108.51036404999999pt height=29.943718200000013pt/>, where <img src="svgs/be2c04c20d96a0fd02628230bf858d13.svg?invert_in_darkmode" align=middle width=41.41163729999999pt height=22.831056599999986pt/> and <img src="svgs/cd7a7aceb8d1f3557d0620ed740a032a.svg?invert_in_darkmode" align=middle width=48.86405534999999pt height=26.76175259999998pt/>.

### Multivariate discrete distributions

While in the univariate case, the target space of a RV is a vector, now it's the cartesian product of the single target spaces: a **multidimensional array**. For example, in bivariate RVs it's a matrix.

The **joint probability mass function** of <img src="svgs/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode" align=middle width=14.908688849999992pt height=22.465723500000017pt/> and <img src="svgs/91aac9730317276af725abd8cef04ca9.svg?invert_in_darkmode" align=middle width=13.19638649999999pt height=22.465723500000017pt/> is defined as <img src="svgs/ffb75b85f2240761ca8bd21c23a8c549.svg?invert_in_darkmode" align=middle width=229.51236164999995pt height=26.584361099999995pt/>, with <img src="svgs/a034342399f7c9e47a099b84188db605.svg?invert_in_darkmode" align=middle width=90.53960849999999pt height=24.65753399999998pt/>, where <img src="svgs/db0dce2a6a38aedb28d33f6650cb22e8.svg?invert_in_darkmode" align=middle width=19.44456194999999pt height=14.15524440000002pt/> is the elemeent of the matrix storing all the possible <img src="svgs/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode" align=middle width=14.908688849999992pt height=22.465723500000017pt/> and <img src="svgs/91aac9730317276af725abd8cef04ca9.svg?invert_in_darkmode" align=middle width=13.19638649999999pt height=22.465723500000017pt/>s, and <img src="svgs/f9c4988898e7f532b9f826a75014ed3c.svg?invert_in_darkmode" align=middle width=14.99998994999999pt height=22.465723500000017pt/> is the total number of matrix elements. We can also write <img src="svgs/d088e2795941aa9f3622f1a3e3f5708b.svg?invert_in_darkmode" align=middle width=30.281021099999993pt height=22.831056599999986pt/> as <img src="svgs/d53011c4c7824fee58ecd4cecbff13ee.svg?invert_in_darkmode" align=middle width=46.406078399999984pt height=24.65753399999998pt/>.

Given the <img src="svgs/bc4ca627cf83d8b37299dae201ef46d4.svg?invert_in_darkmode" align=middle width=30.50360114999999pt height=14.15524440000002pt/> joint probability mass function, we define the **marginal probability distribution** of <img src="svgs/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode" align=middle width=14.908688849999992pt height=22.465723500000017pt/> as <img src="svgs/f5552400bd122d97df9db0e0254c5f89.svg?invert_in_darkmode" align=middle width=242.58132359999996pt height=24.657735299999988pt/> and the one of <img src="svgs/91aac9730317276af725abd8cef04ca9.svg?invert_in_darkmode" align=middle width=13.19638649999999pt height=22.465723500000017pt/> as <img src="svgs/853fd10f3e9451d741e8c6742a65edc7.svg?invert_in_darkmode" align=middle width=241.81309184999995pt height=24.657735299999988pt/>. Note that this is basically just the consideration of the probabilities of a single variable.

### Continuous random variables

Obviously, we can't have PMFs with continuous RVs. What we have, though, are **Probability Density Functions** (PDF), which are basically the continuous version of PMFs: <img src="svgs/d96b7e88cc2ab017bea384bee1a4c9a0.svg?invert_in_darkmode" align=middle width=293.69398244999996pt height=34.33813679999997pt/>. Obviously, when considering the whole target space we'll get <img src="svgs/034d0a6be0424bffe9a6e7ac9236c0f5.svg?invert_in_darkmode" align=middle width=8.219277000000005pt height=21.18732pt/> as always: <img src="svgs/fce2ad8fb0a74a1c1b8caf9ba1b72c19.svg?invert_in_darkmode" align=middle width=98.84008365pt height=34.33813679999997pt/>.

Some examples of PDFs are the *normal distribution* <img src="svgs/aae2d2243d978efbe956b7537dce3cbc.svg?invert_in_darkmode" align=middle width=208.9224687pt height=37.80850590000001pt/>, where <img src="svgs/07617f9d8fe48b4a7b3f523d6730eef0.svg?invert_in_darkmode" align=middle width=9.90492359999999pt height=14.15524440000002pt/> is the expectation and <img src="svgs/8cda31ed38c6d59d14ebefa440099572.svg?invert_in_darkmode" align=middle width=9.98290094999999pt height=14.15524440000002pt/> the standard deviation, and the *exponential distribution* <img src="svgs/7445218cdf75d4078fb8e7082db1c695.svg?invert_in_darkmode" align=middle width=107.63370915pt height=27.91243950000002pt/> which ahs mean <img src="svgs/2f676b30fc526f8d8176780eda0bf7cf.svg?invert_in_darkmode" align=middle width=41.592005399999984pt height=27.77565449999998pt/> and standard deviation <img src="svgs/36497d42de3ed5f325ef1730fa36e1b9.svg?invert_in_darkmode" align=middle width=41.66996294999999pt height=27.77565449999998pt/>.

## Bayes' theorem

Given two RVs <img src="svgs/cbfb1b2a33b28eab8a3e59464768e810.svg?invert_in_darkmode" align=middle width=14.908688849999992pt height=22.465723500000017pt/> and <img src="svgs/91aac9730317276af725abd8cef04ca9.svg?invert_in_darkmode" align=middle width=13.19638649999999pt height=22.465723500000017pt/>, let's consider the instances in which <img src="svgs/e626d8b8793579845f6923033fea5fcc.svg?invert_in_darkmode" align=middle width=46.22128499999999pt height=22.465723500000017pt/>. We can define ***conditional probability*** of <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/> given <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>  <img src="svgs/fc76db86ea6c427fdd05067ba4835daa.svg?invert_in_darkmode" align=middle width=43.66641839999998pt height=24.65753399999998pt/> the fraction of instances for which <img src="svgs/376e59c7cf374a7cabeec3f42b0c0a0b.svg?invert_in_darkmode" align=middle width=43.76323544999999pt height=22.465723500000017pt/>.

Note that the following calculations are made in the *discrete* case: for continuous RVs, substitute sums with integrals.

### Sum rule

Let's consider two discrete RVs <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> and <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/>, we know that <img src="svgs/d68d049717afd0628de886a9841866be.svg?invert_in_darkmode" align=middle width=146.18426459999998pt height=24.657735299999988pt/>, i.e. the probability of <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> happening is the sum of all the probabilities in which <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> happens together with every <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/>: the **marginal distribution** of one rv is the sum on the events of the other rv. If we have more RVs, we can just sum them:  <img src="svgs/199fc9f1375070d07dc5e443a5e97785.svg?invert_in_darkmode" align=middle width=316.4063165999999pt height=24.657735299999988pt/>.

### Product rule

Now, with <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> and <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/>, we can state that <img src="svgs/0c8820ad9001cc2cbf5f176a09f18f55.svg?invert_in_darkmode" align=middle width=142.4411142pt height=24.65753399999998pt/>, i.e. the probability of <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> and <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/> happening is the probability of <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/> happening when <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> has happened, multiplied by the probability of <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> happening. Still pretty obvious.

Let's suppose we have prior on an unobserved variable <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>, and we know the relationship between <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/> and another RV, <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/>, which is observed. We can say that 

<img src="svgs/3f47ffb29b26c2d094d1efcd00aac547.svg?invert_in_darkmode" align=middle width=127.55014964999998pt height=33.20539859999999pt/>, where:

- <img src="svgs/39e18630d38ac07930250653805c861f.svg?invert_in_darkmode" align=middle width=43.66641839999998pt height=24.65753399999998pt/> is called **posterior**, and it is the information we're seeking;
- <img src="svgs/fc76db86ea6c427fdd05067ba4835daa.svg?invert_in_darkmode" align=middle width=43.66641839999998pt height=24.65753399999998pt/> is the **likelihood**, which is the probability of the observed rv, <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/>, given <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>. Basically the inverse of what we're seeking;
- <img src="svgs/c9ea84eb1460d2895e0cf5125bd7f7b5.svg?invert_in_darkmode" align=middle width=30.450987599999987pt height=24.65753399999998pt/> is the **prior**, i.e. knowledge about the discrete probability distribution of <img src="svgs/332cc365a4987aacce0ead01b8bdcc0b.svg?invert_in_darkmode" align=middle width=9.395100000000005pt height=14.155350000000013pt/>, the unobserved RV;
- <img src="svgs/c2f39888dc0b934028add620085faf36.svg?invert_in_darkmode" align=middle width=29.70520574999999pt height=24.65753399999998pt/> is the **evidence**, i.e. the known probability of the observed <img src="svgs/deceeaf6940a8c7a5a02373728002b0f.svg?invert_in_darkmode" align=middle width=8.649300000000004pt height=14.155350000000013pt/>.

Sometimes, calculating the exact posterior can be difficult, maybe because we only got some informations like the mean or the maximum. Some ML algorithms aim exactly at finding the posterior.





